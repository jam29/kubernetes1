* 
* ==> Audit <==
* |--------------|-------------------------|----------|------|---------|----------------------|----------------------|
|   Command    |          Args           | Profile  | User | Version |      Start Time      |       End Time       |
|--------------|-------------------------|----------|------|---------|----------------------|----------------------|
| start        |                         | minikube | jam  | v1.32.0 | 12 Mar 24 16:36 CET  | 12 Mar 24 16:38 CET  |
| ip           |                         | minikube | jam  | v1.32.0 | 12 Mar 24 17:04 CET  | 12 Mar 24 17:04 CET  |
| ip           |                         | minikube | jam  | v1.32.0 | 14 Mar 24 12:19 CET  | 14 Mar 24 12:19 CET  |
| dashboard    |                         | minikube | jam  | v1.32.0 | 18 Mar 24 16:34 CET  |                      |
| update-check |                         | minikube | jam  | v1.32.0 | 28 Jun 24 10:41 CEST | 28 Jun 24 10:41 CEST |
| start        |                         | minikube | jam  | v1.32.0 | 09 Sep 24 11:34 CEST | 09 Sep 24 11:36 CEST |
| stop         |                         | minikube | jam  | v1.32.0 | 09 Sep 24 15:18 CEST | 09 Sep 24 15:18 CEST |
| start        |                         | minikube | jam  | v1.32.0 | 09 Sep 24 15:18 CEST | 09 Sep 24 15:19 CEST |
| kubectl      | -- get pods -A          | minikube | jam  | v1.32.0 | 09 Sep 24 15:19 CEST | 09 Sep 24 15:20 CEST |
| addons       | enable metrics-server   | minikube | jam  | v1.32.0 | 09 Sep 24 15:20 CEST | 09 Sep 24 15:20 CEST |
| ip           |                         | minikube | jam  | v1.32.0 | 09 Sep 24 15:25 CEST | 09 Sep 24 15:25 CEST |
| ip           |                         | minikube | jam  | v1.32.0 | 09 Sep 24 15:27 CEST | 09 Sep 24 15:27 CEST |
| stop         |                         | minikube | jam  | v1.32.0 | 09 Sep 24 15:48 CEST | 09 Sep 24 15:48 CEST |
| start        |                         | minikube | jam  | v1.32.0 | 09 Sep 24 15:50 CEST |                      |
| start        |                         | minikube | jam  | v1.32.0 | 09 Sep 24 15:51 CEST | 09 Sep 24 15:51 CEST |
| ip           |                         | minikube | jam  | v1.32.0 | 09 Sep 24 15:52 CEST | 09 Sep 24 15:52 CEST |
| ssh          |                         | minikube | jam  | v1.32.0 | 09 Sep 24 15:57 CEST | 09 Sep 24 16:51 CEST |
| ssh          |                         | minikube | jam  | v1.32.0 | 09 Sep 24 17:09 CEST | 09 Sep 24 17:18 CEST |
| start        |                         | minikube | jam  | v1.32.0 | 11 Sep 24 15:28 CEST | 11 Sep 24 15:29 CEST |
| ssh          |                         | minikube | jam  | v1.32.0 | 11 Sep 24 15:41 CEST |                      |
| ssh          |                         | minikube | jam  | v1.32.0 | 11 Sep 24 16:33 CEST |                      |
| ip           |                         | minikube | jam  | v1.32.0 | 11 Sep 24 17:14 CEST | 11 Sep 24 17:14 CEST |
| start        |                         | minikube | jam  | v1.32.0 | 17 Sep 24 08:51 CEST | 17 Sep 24 08:52 CEST |
| addons       | enable metrics-server   | minikube | jam  | v1.32.0 | 17 Sep 24 08:53 CEST | 17 Sep 24 08:53 CEST |
| ip           |                         | minikube | jam  | v1.32.0 | 17 Sep 24 08:53 CEST | 17 Sep 24 08:53 CEST |
| ssh          |                         | minikube | jam  | v1.32.0 | 17 Sep 24 08:54 CEST |                      |
| ssh          |                         | minikube | jam  | v1.32.0 | 17 Sep 24 09:15 CEST |                      |
| ssh          |                         | minikube | jam  | v1.32.0 | 17 Sep 24 09:17 CEST | 17 Sep 24 09:26 CEST |
| ssh          |                         | minikube | jam  | v1.32.0 | 17 Sep 24 09:27 CEST | 17 Sep 24 09:28 CEST |
| ip           |                         | minikube | jam  | v1.32.0 | 17 Sep 24 09:38 CEST | 17 Sep 24 09:38 CEST |
| ip           |                         | minikube | jam  | v1.32.0 | 17 Sep 24 09:39 CEST | 17 Sep 24 09:39 CEST |
| ip           |                         | minikube | jam  | v1.32.0 | 17 Sep 24 09:40 CEST | 17 Sep 24 09:40 CEST |
| ip           |                         | minikube | jam  | v1.32.0 | 17 Sep 24 09:45 CEST | 17 Sep 24 09:45 CEST |
| service      | myservice               | minikube | jam  | v1.32.0 | 17 Sep 24 09:54 CEST | 17 Sep 24 10:03 CEST |
| service      | myservice               | minikube | jam  | v1.32.0 | 17 Sep 24 10:04 CEST | 17 Sep 24 10:06 CEST |
| service      | myservice               | minikube | jam  | v1.32.0 | 17 Sep 24 10:06 CEST | 17 Sep 24 11:47 CEST |
| dashboard    |                         | minikube | jam  | v1.32.0 | 17 Sep 24 13:18 CEST |                      |
| start        |                         | minikube | jam  | v1.32.0 | 17 Sep 24 13:23 CEST | 17 Sep 24 13:24 CEST |
| update-check |                         | minikube | jam  | v1.32.0 | 17 Sep 24 13:32 CEST | 17 Sep 24 13:32 CEST |
| update-check |                         | minikube | jam  | v1.32.0 | 17 Sep 24 15:02 CEST | 17 Sep 24 15:02 CEST |
| update-check |                         | minikube | jam  | v1.32.0 | 17 Sep 24 15:06 CEST | 17 Sep 24 15:06 CEST |
| stop         |                         | minikube | jam  | v1.32.0 | 17 Sep 24 16:41 CEST | 17 Sep 24 16:41 CEST |
| start        | --nodes=2 --cni=flannel | minikube | jam  | v1.32.0 | 17 Sep 24 16:41 CEST | 17 Sep 24 16:42 CEST |
| addons       | enable kubevirt         | minikube | jam  | v1.32.0 | 17 Sep 24 16:43 CEST | 17 Sep 24 16:43 CEST |
| stop         |                         | minikube | jam  | v1.32.0 | 17 Sep 24 16:44 CEST | 17 Sep 24 16:44 CEST |
| start        |                         | minikube | jam  | v1.32.0 | 18 Sep 24 00:58 CEST | 18 Sep 24 00:59 CEST |
| service      | k8bis                   | minikube | jam  | v1.32.0 | 18 Sep 24 08:30 CEST | 18 Sep 24 09:43 CEST |
| update-check |                         | minikube | jam  | v1.32.0 | 18 Sep 24 09:10 CEST | 18 Sep 24 09:10 CEST |
| service      | asso1                   | minikube | jam  | v1.32.0 | 18 Sep 24 09:30 CEST | 18 Sep 24 09:31 CEST |
| service      | asso1                   | minikube | jam  | v1.32.0 | 18 Sep 24 09:41 CEST | 18 Sep 24 09:45 CEST |
| service      | k8bis                   | minikube | jam  | v1.32.0 | 18 Sep 24 09:43 CEST |                      |
| service      | k8bis                   | minikube | jam  | v1.32.0 | 18 Sep 24 09:44 CEST |                      |
| service      | k8bis                   | minikube | jam  | v1.32.0 | 18 Sep 24 09:44 CEST |                      |
| service      | asso1                   | minikube | jam  | v1.32.0 | 18 Sep 24 09:46 CEST | 18 Sep 24 09:47 CEST |
| service      | asso1                   | minikube | jam  | v1.32.0 | 18 Sep 24 09:54 CEST | 18 Sep 24 09:57 CEST |
| service      | asso1                   | minikube | jam  | v1.32.0 | 18 Sep 24 09:58 CEST |                      |
|--------------|-------------------------|----------|------|---------|----------------------|----------------------|

* 
* ==> Dernier d√©marrage <==
* Log file created at: 2024/09/18 00:58:11
Running on machine: Macintosh
Binary: Built with gc go1.21.3 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0918 00:58:11.314831   52240 out.go:296] Setting OutFile to fd 1 ...
I0918 00:58:11.315979   52240 out.go:348] isatty.IsTerminal(1) = true
I0918 00:58:11.315988   52240 out.go:309] Setting ErrFile to fd 2...
I0918 00:58:11.315995   52240 out.go:348] isatty.IsTerminal(2) = true
I0918 00:58:11.317396   52240 root.go:338] Updating PATH: /Users/jam/.minikube/bin
W0918 00:58:11.318917   52240 root.go:314] Error reading config file at /Users/jam/.minikube/config/config.json: open /Users/jam/.minikube/config/config.json: no such file or directory
I0918 00:58:11.328670   52240 out.go:303] Setting JSON to false
I0918 00:58:11.422760   52240 start.go:128] hostinfo: {"hostname":"Macintosh","uptime":125992,"bootTime":1726487899,"procs":594,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"11.7.10","kernelVersion":"20.6.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"5b8ab005-5481-5ae9-b294-be0f140402a1"}
W0918 00:58:11.424106   52240 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0918 00:58:11.435025   52240 out.go:177] üòÑ  minikube v1.32.0 sur Darwin 11.7.10
I0918 00:58:11.448877   52240 notify.go:220] Checking for updates...
I0918 00:58:11.449470   52240 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0918 00:58:11.450190   52240 driver.go:378] Setting default libvirt URI to qemu:///system
I0918 00:58:12.020115   52240 docker.go:122] docker version: linux-20.10.12:Docker Engine - Community
I0918 00:58:12.020556   52240 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0918 00:58:13.384481   52240 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.363902526s)
I0918 00:58:13.386184   52240 info.go:266] docker info: {ID:MX54:3H3R:T6UP:DY4Q:3BQP:PNLH:KDCQ:27UR:N224:RVTA:ACLQ:Q5JH Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:true NFd:48 OomKillDisable:false NGoroutines:49 SystemTime:2024-09-17 22:58:12.3781296 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.76-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:5176152064 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:docker-desktop Labels:[] ExperimentalBuild:true ServerVersion:20.10.12 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7b11cfaabd73bb80907dd23182b9347b4245eb5d Expected:7b11cfaabd73bb80907dd23182b9347b4245eb5d} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.7.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.2.3] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.16.0]] Warnings:<nil>}}
I0918 00:58:13.396887   52240 out.go:177] ‚ú®  Utilisation du pilote docker bas√© sur le profil existant
I0918 00:58:13.405336   52240 start.go:298] selected driver: docker
I0918 00:58:13.405388   52240 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:flannel NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true kubevirt:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0918 00:58:13.405647   52240 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0918 00:58:13.406805   52240 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0918 00:58:14.041319   52240 info.go:266] docker info: {ID:MX54:3H3R:T6UP:DY4Q:3BQP:PNLH:KDCQ:27UR:N224:RVTA:ACLQ:Q5JH Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:9 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:true NFd:48 OomKillDisable:false NGoroutines:49 SystemTime:2024-09-17 22:58:13.8261118 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.76-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:5176152064 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:docker-desktop Labels:[] ExperimentalBuild:true ServerVersion:20.10.12 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7b11cfaabd73bb80907dd23182b9347b4245eb5d Expected:7b11cfaabd73bb80907dd23182b9347b4245eb5d} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.7.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.2.3] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.16.0]] Warnings:<nil>}}
I0918 00:58:14.060400   52240 cni.go:84] Creating CNI manager for "flannel"
I0918 00:58:14.060481   52240 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:flannel NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true kubevirt:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0918 00:58:14.077909   52240 out.go:177] üëç  D√©marrage du noeud de plan de contr√¥le minikube dans le cluster minikube
I0918 00:58:14.083569   52240 cache.go:121] Beginning downloading kic base image for docker with docker
I0918 00:58:14.088414   52240 out.go:177] üöú  Extraction de l'image de base...
I0918 00:58:14.098181   52240 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0918 00:58:14.098263   52240 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0918 00:58:14.098443   52240 preload.go:148] Found local preload: /Users/jam/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0918 00:58:14.098465   52240 cache.go:56] Caching tarball of preloaded images
I0918 00:58:14.100383   52240 preload.go:174] Found /Users/jam/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0918 00:58:14.100547   52240 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0918 00:58:14.101059   52240 profile.go:148] Saving config to /Users/jam/.minikube/profiles/minikube/config.json ...
I0918 00:58:14.499711   52240 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0918 00:58:14.500285   52240 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0918 00:58:14.500359   52240 cache.go:194] Successfully downloaded all kic artifacts
I0918 00:58:14.500811   52240 start.go:365] acquiring machines lock for minikube: {Name:mk36424a4599cbaa53cfbbd72241b5b021076a95 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0918 00:58:14.501408   52240 start.go:369] acquired machines lock for "minikube" in 547.692¬µs
I0918 00:58:14.501475   52240 start.go:96] Skipping create...Using existing machine configuration
I0918 00:58:14.501493   52240 fix.go:54] fixHost starting: 
I0918 00:58:14.502171   52240 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 00:58:14.909812   52240 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0918 00:58:14.909854   52240 fix.go:128] unexpected machine state, will restart: <nil>
I0918 00:58:14.921774   52240 out.go:177] üîÑ  Red√©marrage du docker container existant pour "minikube" ...
I0918 00:58:14.927493   52240 cli_runner.go:164] Run: docker start minikube
I0918 00:58:17.850653   52240 cli_runner.go:217] Completed: docker start minikube: (2.923143966s)
I0918 00:58:17.851027   52240 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 00:58:18.416969   52240 kic.go:430] container "minikube" state is running.
I0918 00:58:18.420008   52240 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0918 00:58:18.958689   52240 profile.go:148] Saving config to /Users/jam/.minikube/profiles/minikube/config.json ...
I0918 00:58:18.960594   52240 machine.go:88] provisioning docker machine ...
I0918 00:58:18.960685   52240 ubuntu.go:169] provisioning hostname "minikube"
I0918 00:58:18.961109   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:58:19.491655   52240 main.go:141] libmachine: Using SSH client type: native
I0918 00:58:19.494082   52240 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 57551 <nil> <nil>}
I0918 00:58:19.494134   52240 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0918 00:58:19.515808   52240 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0918 00:58:22.951672   52240 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0918 00:58:22.951929   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:58:23.446050   52240 main.go:141] libmachine: Using SSH client type: native
I0918 00:58:23.446583   52240 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 57551 <nil> <nil>}
I0918 00:58:23.446598   52240 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0918 00:58:23.678053   52240 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0918 00:58:23.678113   52240 ubuntu.go:175] set auth options {CertDir:/Users/jam/.minikube CaCertPath:/Users/jam/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/jam/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/jam/.minikube/machines/server.pem ServerKeyPath:/Users/jam/.minikube/machines/server-key.pem ClientKeyPath:/Users/jam/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/jam/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/jam/.minikube}
I0918 00:58:23.678158   52240 ubuntu.go:177] setting up certificates
I0918 00:58:23.678653   52240 provision.go:83] configureAuth start
I0918 00:58:23.679111   52240 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0918 00:58:24.002446   52240 provision.go:138] copyHostCerts
I0918 00:58:24.004348   52240 exec_runner.go:144] found /Users/jam/.minikube/ca.pem, removing ...
I0918 00:58:24.004794   52240 exec_runner.go:203] rm: /Users/jam/.minikube/ca.pem
I0918 00:58:24.005358   52240 exec_runner.go:151] cp: /Users/jam/.minikube/certs/ca.pem --> /Users/jam/.minikube/ca.pem (1070 bytes)
I0918 00:58:24.007746   52240 exec_runner.go:144] found /Users/jam/.minikube/cert.pem, removing ...
I0918 00:58:24.007769   52240 exec_runner.go:203] rm: /Users/jam/.minikube/cert.pem
I0918 00:58:24.008008   52240 exec_runner.go:151] cp: /Users/jam/.minikube/certs/cert.pem --> /Users/jam/.minikube/cert.pem (1115 bytes)
I0918 00:58:24.009273   52240 exec_runner.go:144] found /Users/jam/.minikube/key.pem, removing ...
I0918 00:58:24.009283   52240 exec_runner.go:203] rm: /Users/jam/.minikube/key.pem
I0918 00:58:24.009460   52240 exec_runner.go:151] cp: /Users/jam/.minikube/certs/key.pem --> /Users/jam/.minikube/key.pem (1675 bytes)
I0918 00:58:24.010062   52240 provision.go:112] generating server cert: /Users/jam/.minikube/machines/server.pem ca-key=/Users/jam/.minikube/certs/ca.pem private-key=/Users/jam/.minikube/certs/ca-key.pem org=jam.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0918 00:58:24.534108   52240 provision.go:172] copyRemoteCerts
I0918 00:58:24.535479   52240 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0918 00:58:24.535615   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:58:24.785692   52240 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57551 SSHKeyPath:/Users/jam/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:58:24.897876   52240 ssh_runner.go:362] scp /Users/jam/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I0918 00:58:24.943205   52240 ssh_runner.go:362] scp /Users/jam/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0918 00:58:24.981793   52240 ssh_runner.go:362] scp /Users/jam/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0918 00:58:25.019314   52240 provision.go:86] duration metric: configureAuth took 1.340683524s
I0918 00:58:25.019337   52240 ubuntu.go:193] setting minikube options for container-runtime
I0918 00:58:25.020459   52240 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0918 00:58:25.020589   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:58:25.239769   52240 main.go:141] libmachine: Using SSH client type: native
I0918 00:58:25.240292   52240 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 57551 <nil> <nil>}
I0918 00:58:25.240301   52240 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0918 00:58:25.402476   52240 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0918 00:58:25.402492   52240 ubuntu.go:71] root file system type: overlay
I0918 00:58:25.402714   52240 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0918 00:58:25.402957   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:58:25.652307   52240 main.go:141] libmachine: Using SSH client type: native
I0918 00:58:25.653351   52240 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 57551 <nil> <nil>}
I0918 00:58:25.653482   52240 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0918 00:58:25.827186   52240 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0918 00:58:25.827712   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:58:26.105517   52240 main.go:141] libmachine: Using SSH client type: native
I0918 00:58:26.106403   52240 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1405ca0] 0x1408980 <nil>  [] 0s} 127.0.0.1 57551 <nil> <nil>}
I0918 00:58:26.106424   52240 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0918 00:58:26.280132   52240 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0918 00:58:26.280149   52240 machine.go:91] provisioned docker machine in 7.319815433s
I0918 00:58:26.280156   52240 start.go:300] post-start starting for "minikube" (driver="docker")
I0918 00:58:26.280170   52240 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0918 00:58:26.280325   52240 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0918 00:58:26.280479   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:58:26.517602   52240 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57551 SSHKeyPath:/Users/jam/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:58:26.637571   52240 ssh_runner.go:195] Run: cat /etc/os-release
I0918 00:58:26.644649   52240 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0918 00:58:26.644673   52240 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0918 00:58:26.644680   52240 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0918 00:58:26.644684   52240 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0918 00:58:26.644693   52240 filesync.go:126] Scanning /Users/jam/.minikube/addons for local assets ...
I0918 00:58:26.645046   52240 filesync.go:126] Scanning /Users/jam/.minikube/files for local assets ...
I0918 00:58:26.645277   52240 start.go:303] post-start completed in 365.128271ms
I0918 00:58:26.645466   52240 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0918 00:58:26.645554   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:58:26.844544   52240 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57551 SSHKeyPath:/Users/jam/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:58:26.942726   52240 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0918 00:58:26.951011   52240 fix.go:56] fixHost completed within 12.449983805s
I0918 00:58:26.951032   52240 start.go:83] releasing machines lock for "minikube", held for 12.450069649s
I0918 00:58:26.951254   52240 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0918 00:58:27.130183   52240 ssh_runner.go:195] Run: cat /version.json
I0918 00:58:27.130557   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:58:27.130897   52240 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0918 00:58:27.131929   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:58:27.372859   52240 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57551 SSHKeyPath:/Users/jam/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:58:27.393288   52240 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57551 SSHKeyPath:/Users/jam/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:58:27.730861   52240 ssh_runner.go:195] Run: systemctl --version
I0918 00:58:27.740404   52240 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0918 00:58:27.753268   52240 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0918 00:58:27.790347   52240 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0918 00:58:27.790631   52240 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0918 00:58:27.804553   52240 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0918 00:58:27.804577   52240 start.go:472] detecting cgroup driver to use...
I0918 00:58:27.804599   52240 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0918 00:58:27.804814   52240 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0918 00:58:27.831271   52240 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0918 00:58:27.849518   52240 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0918 00:58:27.868297   52240 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0918 00:58:27.868520   52240 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0918 00:58:27.883977   52240 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0918 00:58:27.901457   52240 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0918 00:58:27.919990   52240 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0918 00:58:27.938702   52240 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0918 00:58:27.955720   52240 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0918 00:58:27.973133   52240 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0918 00:58:27.989872   52240 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0918 00:58:28.004744   52240 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0918 00:58:28.108811   52240 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0918 00:58:28.239736   52240 start.go:472] detecting cgroup driver to use...
I0918 00:58:28.239783   52240 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0918 00:58:28.240057   52240 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0918 00:58:28.262189   52240 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0918 00:58:28.262352   52240 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0918 00:58:28.280587   52240 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0918 00:58:28.307324   52240 ssh_runner.go:195] Run: which cri-dockerd
I0918 00:58:28.315617   52240 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0918 00:58:28.329739   52240 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0918 00:58:28.361889   52240 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0918 00:58:28.541072   52240 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0918 00:58:28.699077   52240 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0918 00:58:28.699211   52240 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0918 00:58:28.725693   52240 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0918 00:58:28.831031   52240 ssh_runner.go:195] Run: sudo systemctl restart docker
I0918 00:58:29.470311   52240 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0918 00:58:29.563023   52240 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0918 00:58:29.659025   52240 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0918 00:58:29.757333   52240 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0918 00:58:29.870479   52240 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0918 00:58:29.912413   52240 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0918 00:58:30.005736   52240 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0918 00:58:30.206259   52240 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0918 00:58:30.206748   52240 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0918 00:58:30.215287   52240 start.go:540] Will wait 60s for crictl version
I0918 00:58:30.215456   52240 ssh_runner.go:195] Run: which crictl
I0918 00:58:30.224443   52240 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0918 00:58:30.378772   52240 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0918 00:58:30.378930   52240 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0918 00:58:30.468337   52240 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0918 00:58:30.516910   52240 out.go:204] üê≥  Pr√©paration de Kubernetes v1.28.3 sur Docker 24.0.7...
I0918 00:58:30.517080   52240 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0918 00:58:30.852579   52240 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I0918 00:58:30.853245   52240 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I0918 00:58:30.862828   52240 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0918 00:58:30.880007   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0918 00:58:31.063162   52240 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0918 00:58:31.063279   52240 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0918 00:58:31.096285   52240 docker.go:671] Got preloaded images: -- stdout --
mongo:latest
nginx:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
flannel/flannel:v0.22.3
registry.k8s.io/metrics-server/metrics-server:<none>
flannel/flannel-cni-plugin:v1.2.0
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
bitnami/kubectl:<none>
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0918 00:58:31.096324   52240 docker.go:601] Images already preloaded, skipping extraction
I0918 00:58:31.096476   52240 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0918 00:58:31.128753   52240 docker.go:671] Got preloaded images: -- stdout --
mongo:latest
nginx:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
flannel/flannel:v0.22.3
registry.k8s.io/metrics-server/metrics-server:<none>
flannel/flannel-cni-plugin:v1.2.0
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
bitnami/kubectl:<none>
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0918 00:58:31.128771   52240 cache_images.go:84] Images are preloaded, skipping loading
I0918 00:58:31.128929   52240 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0918 00:58:31.315169   52240 cni.go:84] Creating CNI manager for "flannel"
I0918 00:58:31.319683   52240 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0918 00:58:31.319708   52240 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0918 00:58:31.319861   52240 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0918 00:58:31.319961   52240 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:flannel NodeIP: NodePort:8443 NodeName:}
I0918 00:58:31.320105   52240 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0918 00:58:31.338488   52240 binaries.go:44] Found k8s binaries, skipping transfer
I0918 00:58:31.338789   52240 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0918 00:58:31.354754   52240 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0918 00:58:31.381113   52240 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0918 00:58:31.405437   52240 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0918 00:58:31.430274   52240 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0918 00:58:31.436749   52240 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0918 00:58:31.453940   52240 certs.go:56] Setting up /Users/jam/.minikube/profiles/minikube for IP: 192.168.49.2
I0918 00:58:31.453964   52240 certs.go:190] acquiring lock for shared ca certs: {Name:mka32162b9ec92b36463d5ea8bc8d96aa36d34c5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 00:58:31.455918   52240 certs.go:199] skipping minikubeCA CA generation: /Users/jam/.minikube/ca.key
I0918 00:58:31.456629   52240 certs.go:199] skipping proxyClientCA CA generation: /Users/jam/.minikube/proxy-client-ca.key
I0918 00:58:31.457640   52240 certs.go:315] skipping minikube-user signed cert generation: /Users/jam/.minikube/profiles/minikube/client.key
I0918 00:58:31.458702   52240 certs.go:315] skipping minikube signed cert generation: /Users/jam/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0918 00:58:31.459478   52240 certs.go:315] skipping aggregator signed cert generation: /Users/jam/.minikube/profiles/minikube/proxy-client.key
I0918 00:58:31.460397   52240 certs.go:437] found cert: /Users/jam/.minikube/certs/Users/jam/.minikube/certs/ca-key.pem (1679 bytes)
I0918 00:58:31.460470   52240 certs.go:437] found cert: /Users/jam/.minikube/certs/Users/jam/.minikube/certs/ca.pem (1070 bytes)
I0918 00:58:31.460527   52240 certs.go:437] found cert: /Users/jam/.minikube/certs/Users/jam/.minikube/certs/cert.pem (1115 bytes)
I0918 00:58:31.460579   52240 certs.go:437] found cert: /Users/jam/.minikube/certs/Users/jam/.minikube/certs/key.pem (1675 bytes)
I0918 00:58:31.461709   52240 ssh_runner.go:362] scp /Users/jam/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0918 00:58:31.500333   52240 ssh_runner.go:362] scp /Users/jam/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0918 00:58:31.532136   52240 ssh_runner.go:362] scp /Users/jam/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0918 00:58:31.568916   52240 ssh_runner.go:362] scp /Users/jam/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0918 00:58:31.605137   52240 ssh_runner.go:362] scp /Users/jam/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0918 00:58:31.639067   52240 ssh_runner.go:362] scp /Users/jam/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0918 00:58:31.672850   52240 ssh_runner.go:362] scp /Users/jam/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0918 00:58:31.706337   52240 ssh_runner.go:362] scp /Users/jam/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0918 00:58:31.739144   52240 ssh_runner.go:362] scp /Users/jam/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0918 00:58:31.771512   52240 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0918 00:58:31.799508   52240 ssh_runner.go:195] Run: openssl version
I0918 00:58:31.810385   52240 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0918 00:58:31.825049   52240 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0918 00:58:31.832055   52240 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Mar 12  2024 /usr/share/ca-certificates/minikubeCA.pem
I0918 00:58:31.832187   52240 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0918 00:58:31.843141   52240 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0918 00:58:31.856584   52240 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0918 00:58:31.865509   52240 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0918 00:58:31.876903   52240 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0918 00:58:31.887464   52240 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0918 00:58:31.897385   52240 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0918 00:58:31.909534   52240 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0918 00:58:31.921329   52240 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0918 00:58:31.932823   52240 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:flannel NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true kubevirt:true metrics-server:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0918 00:58:31.932964   52240 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0918 00:58:31.967652   52240 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0918 00:58:31.985458   52240 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0918 00:58:31.985493   52240 kubeadm.go:636] restartCluster start
I0918 00:58:31.985609   52240 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0918 00:58:32.000873   52240 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0918 00:58:32.000996   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0918 00:58:32.187163   52240 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in /Users/jam/.kube/config
I0918 00:58:32.187617   52240 kubeconfig.go:146] "minikube" context is missing from /Users/jam/.kube/config - will repair!
I0918 00:58:32.188272   52240 lock.go:35] WriteFile acquiring /Users/jam/.kube/config: {Name:mk463ae0c7305f72a4c9263b2631e5e2473b8632 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 00:58:32.208188   52240 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0918 00:58:32.224285   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:32.224434   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:32.241751   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:32.241764   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:32.241931   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:32.258483   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:32.763501   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:32.763762   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:32.783584   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:33.262327   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:33.262578   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:33.281280   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:33.762286   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:33.762605   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:33.782569   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:34.263217   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:34.263480   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:34.283090   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:34.762197   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:34.762418   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:34.781862   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:35.259355   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:35.259644   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:35.281727   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:35.762243   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:35.762508   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:35.782183   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:36.262663   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:36.263031   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:36.300716   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:36.763356   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:36.764023   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:36.796789   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:37.260995   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:37.261223   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:37.291015   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:37.760881   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:37.761084   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:37.799775   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:38.262584   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:38.262849   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:38.295317   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:38.761162   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:38.761501   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:38.795516   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:39.260995   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:39.261179   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:39.285966   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:39.762061   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:39.762478   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:39.798304   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:40.261937   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:40.262107   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:40.292894   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:40.758297   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:40.758489   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:40.790132   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:41.260437   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:41.260603   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:41.294893   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:41.762885   52240 api_server.go:166] Checking apiserver status ...
I0918 00:58:41.763069   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0918 00:58:41.787440   52240 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0918 00:58:42.227509   52240 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0918 00:58:42.227554   52240 kubeadm.go:1128] stopping kube-system containers ...
I0918 00:58:42.227904   52240 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0918 00:58:42.292936   52240 docker.go:469] Stopping containers: [e7848b234837 8af58c4646d7 35d68174d762 7e1e9934d2c4 bb2deb81b0e6 8ee185e810c7 756abc953a48 bf2d1805e1f3 77f04ce3ebd4 8087e7a1fe96 9012d0c5ef66 e8bd370ad18e ccedc038ab1a cc443522d7d8 a42cabb84c38 e09eb81e9b42 80d2544fc889 5bcae6a219a7 d1c03cc2a9d3 b793273de709 17eee246ed2c dee7b418d050 2057f5d05956 bfcf3bdb587f c09d68abece4 42b72500becd 42ee6e377f24 0438fa3d6f42 ff936cd1e6e3 487ca5e217a3 401630ee08d2 bcb101f711e9 150071680c8b]
I0918 00:58:42.293289   52240 ssh_runner.go:195] Run: docker stop e7848b234837 8af58c4646d7 35d68174d762 7e1e9934d2c4 bb2deb81b0e6 8ee185e810c7 756abc953a48 bf2d1805e1f3 77f04ce3ebd4 8087e7a1fe96 9012d0c5ef66 e8bd370ad18e ccedc038ab1a cc443522d7d8 a42cabb84c38 e09eb81e9b42 80d2544fc889 5bcae6a219a7 d1c03cc2a9d3 b793273de709 17eee246ed2c dee7b418d050 2057f5d05956 bfcf3bdb587f c09d68abece4 42b72500becd 42ee6e377f24 0438fa3d6f42 ff936cd1e6e3 487ca5e217a3 401630ee08d2 bcb101f711e9 150071680c8b
I0918 00:58:42.375498   52240 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0918 00:58:42.409140   52240 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0918 00:58:42.439533   52240 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Sep  9 09:36 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Sep 17 14:42 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Sep  9 09:36 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Sep 17 14:42 /etc/kubernetes/scheduler.conf

I0918 00:58:42.440117   52240 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0918 00:58:42.467757   52240 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0918 00:58:42.489127   52240 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0918 00:58:42.504339   52240 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0918 00:58:42.504446   52240 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0918 00:58:42.517945   52240 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0918 00:58:42.533698   52240 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0918 00:58:42.533828   52240 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0918 00:58:42.563154   52240 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0918 00:58:42.599932   52240 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0918 00:58:42.599951   52240 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0918 00:58:42.885054   52240 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0918 00:58:44.251752   52240 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.366697434s)
I0918 00:58:44.251789   52240 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0918 00:58:44.484611   52240 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0918 00:58:44.648129   52240 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0918 00:58:44.794069   52240 api_server.go:52] waiting for apiserver process to appear ...
I0918 00:58:44.794237   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:58:44.830462   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:58:45.359568   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:58:45.859878   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:58:46.357283   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:58:46.858423   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:58:47.357537   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:58:47.858346   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:58:48.358683   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:58:48.858411   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:58:49.358484   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:58:49.419823   52240 api_server.go:72] duration metric: took 4.625897404s to wait for apiserver process to appear ...
I0918 00:58:49.419887   52240 api_server.go:88] waiting for apiserver healthz status ...
I0918 00:58:49.420019   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:58:49.429105   52240 api_server.go:269] stopped: https://127.0.0.1:57550/healthz: Get "https://127.0.0.1:57550/healthz": EOF
I0918 00:58:49.429164   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:58:49.432476   52240 api_server.go:269] stopped: https://127.0.0.1:57550/healthz: Get "https://127.0.0.1:57550/healthz": EOF
I0918 00:58:49.936289   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:58:49.942554   52240 api_server.go:269] stopped: https://127.0.0.1:57550/healthz: Get "https://127.0.0.1:57550/healthz": EOF
I0918 00:58:50.436266   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:58:50.444532   52240 api_server.go:269] stopped: https://127.0.0.1:57550/healthz: Get "https://127.0.0.1:57550/healthz": EOF
I0918 00:58:50.936197   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:58:55.936616   52240 api_server.go:269] stopped: https://127.0.0.1:57550/healthz: Get "https://127.0.0.1:57550/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0918 00:58:55.936840   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:58:59.393066   52240 api_server.go:279] https://127.0.0.1:57550/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0918 00:58:59.393109   52240 api_server.go:103] status: https://127.0.0.1:57550/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0918 00:58:59.393143   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:58:59.973117   52240 api_server.go:279] https://127.0.0.1:57550/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0918 00:58:59.973143   52240 api_server.go:103] status: https://127.0.0.1:57550/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0918 00:58:59.973168   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:59:00.163871   52240 api_server.go:279] https://127.0.0.1:57550/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0918 00:59:00.163914   52240 api_server.go:103] status: https://127.0.0.1:57550/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0918 00:59:00.436949   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:59:00.585632   52240 api_server.go:279] https://127.0.0.1:57550/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0918 00:59:00.585683   52240 api_server.go:103] status: https://127.0.0.1:57550/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0918 00:59:00.935782   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:59:00.966612   52240 api_server.go:279] https://127.0.0.1:57550/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0918 00:59:00.966644   52240 api_server.go:103] status: https://127.0.0.1:57550/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0918 00:59:01.436067   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:59:01.464724   52240 api_server.go:279] https://127.0.0.1:57550/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0918 00:59:01.464752   52240 api_server.go:103] status: https://127.0.0.1:57550/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0918 00:59:01.936062   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:59:01.972126   52240 api_server.go:279] https://127.0.0.1:57550/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0918 00:59:01.972149   52240 api_server.go:103] status: https://127.0.0.1:57550/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0918 00:59:02.434494   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:59:02.458410   52240 api_server.go:279] https://127.0.0.1:57550/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0918 00:59:02.458434   52240 api_server.go:103] status: https://127.0.0.1:57550/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0918 00:59:02.936987   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:59:02.965192   52240 api_server.go:279] https://127.0.0.1:57550/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0918 00:59:02.965225   52240 api_server.go:103] status: https://127.0.0.1:57550/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0918 00:59:03.435680   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:59:03.455942   52240 api_server.go:279] https://127.0.0.1:57550/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0918 00:59:03.455967   52240 api_server.go:103] status: https://127.0.0.1:57550/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0918 00:59:03.936577   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:59:03.957572   52240 api_server.go:279] https://127.0.0.1:57550/healthz returned 200:
ok
I0918 00:59:03.979737   52240 api_server.go:141] control plane version: v1.28.3
I0918 00:59:03.979766   52240 api_server.go:131] duration metric: took 14.560406307s to wait for apiserver health ...
I0918 00:59:03.979780   52240 cni.go:84] Creating CNI manager for "flannel"
I0918 00:59:03.987709   52240 out.go:177] üîó  Configuration de Flannel (Container Networking Interface)...
I0918 00:59:03.993264   52240 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I0918 00:59:04.058181   52240 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.28.3/kubectl ...
I0918 00:59:04.058189   52240 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (4398 bytes)
I0918 00:59:04.265098   52240 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0918 00:59:30.594045   52240 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml: (26.329900621s)
I0918 00:59:30.594081   52240 system_pods.go:43] waiting for kube-system pods to appear ...
I0918 00:59:30.614476   52240 system_pods.go:59] 9 kube-system pods found
I0918 00:59:30.614491   52240 system_pods.go:61] "coredns-5dd5756b68-2sqx9" [cc684e72-bf1a-4286-a82e-9a8e7f8f5e81] Running
I0918 00:59:30.614504   52240 system_pods.go:61] "etcd-minikube" [324f41ac-5401-4bd3-b114-6bc033901b0a] Running
I0918 00:59:30.614507   52240 system_pods.go:61] "kube-apiserver-minikube" [9a97f7a8-8dce-46b0-892d-69c46521a95f] Running
I0918 00:59:30.614510   52240 system_pods.go:61] "kube-controller-manager-minikube" [6f79b6f3-bdc5-490f-bdcf-da5d78e2b4f2] Running
I0918 00:59:30.614518   52240 system_pods.go:61] "kube-proxy-48xsd" [31824f72-597d-4319-8f37-075475c4edfe] Running
I0918 00:59:30.614521   52240 system_pods.go:61] "kube-scheduler-minikube" [6db0797c-020c-4b63-87f2-ae9a93a07084] Running
I0918 00:59:30.614525   52240 system_pods.go:61] "kubevirt-install-manager" [6b4ec9e3-7c91-4b92-b911-ebcb1331c6da] Running
I0918 00:59:30.614528   52240 system_pods.go:61] "metrics-server-7c66d45ddc-ffjlk" [1b350d94-9de9-4cf7-a758-e66fd169fedb] Running
I0918 00:59:30.614530   52240 system_pods.go:61] "storage-provisioner" [688c3951-205d-4398-a857-a0822361a6c3] Running
I0918 00:59:30.614544   52240 system_pods.go:74] duration metric: took 20.449286ms to wait for pod list to return data ...
I0918 00:59:30.614549   52240 node_conditions.go:102] verifying NodePressure condition ...
I0918 00:59:30.622538   52240 node_conditions.go:122] node storage ephemeral capacity is 61252420Ki
I0918 00:59:30.622552   52240 node_conditions.go:123] node cpu capacity is 4
I0918 00:59:30.622561   52240 node_conditions.go:105] duration metric: took 8.009268ms to run NodePressure ...
I0918 00:59:30.622579   52240 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0918 00:59:30.976710   52240 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0918 00:59:30.993987   52240 ops.go:34] apiserver oom_adj: -16
I0918 00:59:30.993996   52240 kubeadm.go:640] restartCluster took 59.010686691s
I0918 00:59:30.994005   52240 kubeadm.go:406] StartCluster complete in 59.063383166s
I0918 00:59:30.994023   52240 settings.go:142] acquiring lock: {Name:mk8eeb7be67815de3da74dc5314bb300369823fe Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 00:59:30.994420   52240 settings.go:150] Updating kubeconfig:  /Users/jam/.kube/config
I0918 00:59:30.997302   52240 lock.go:35] WriteFile acquiring /Users/jam/.kube/config: {Name:mk463ae0c7305f72a4c9263b2631e5e2473b8632 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0918 00:59:30.997705   52240 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0918 00:59:30.997784   52240 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:true logviewer:false metallb:false metrics-server:true nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0918 00:59:30.997901   52240 addons.go:69] Setting kubevirt=true in profile "minikube"
I0918 00:59:30.997907   52240 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0918 00:59:30.997939   52240 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0918 00:59:30.997959   52240 addons.go:69] Setting dashboard=true in profile "minikube"
I0918 00:59:30.997980   52240 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0918 00:59:30.998027   52240 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0918 00:59:30.998045   52240 addons.go:69] Setting metrics-server=true in profile "minikube"
I0918 00:59:30.998119   52240 addons.go:231] Setting addon kubevirt=true in "minikube"
I0918 00:59:30.998120   52240 addons.go:231] Setting addon metrics-server=true in "minikube"
W0918 00:59:30.998126   52240 addons.go:240] addon kubevirt should already be in state true
I0918 00:59:30.998129   52240 addons.go:231] Setting addon dashboard=true in "minikube"
W0918 00:59:30.998132   52240 addons.go:240] addon metrics-server should already be in state true
I0918 00:59:30.998131   52240 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0918 00:59:30.998151   52240 addons.go:240] addon dashboard should already be in state true
W0918 00:59:30.998154   52240 addons.go:240] addon storage-provisioner should already be in state true
I0918 00:59:30.998217   52240 host.go:66] Checking if "minikube" exists ...
I0918 00:59:30.998901   52240 host.go:66] Checking if "minikube" exists ...
I0918 00:59:30.998921   52240 host.go:66] Checking if "minikube" exists ...
I0918 00:59:30.998924   52240 host.go:66] Checking if "minikube" exists ...
I0918 00:59:30.999965   52240 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 00:59:31.000223   52240 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 00:59:31.000249   52240 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 00:59:31.000302   52240 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 00:59:31.000320   52240 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 00:59:31.025772   52240 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0918 00:59:31.025867   52240 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0918 00:59:31.032969   52240 out.go:177] üîé  V√©rification des composants Kubernetes...
I0918 00:59:31.046970   52240 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0918 00:59:31.351967   52240 out.go:177]     ‚ñ™ Utilisation de l'image docker.io/bitnami/kubectl:1.24.7
I0918 00:59:31.356091   52240 out.go:177]     ‚ñ™ Utilisation de l'image gcr.io/k8s-minikube/storage-provisioner:v5
I0918 00:59:31.360668   52240 addons.go:423] installing /etc/kubernetes/addons/pod.yaml
I0918 00:59:31.364922   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/pod.yaml (2413 bytes)
I0918 00:59:31.369835   52240 out.go:177]     ‚ñ™ Utilisation de l'image docker.io/kubernetesui/dashboard:v2.7.0
I0918 00:59:31.363869   52240 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0918 00:59:31.369870   52240 addons.go:240] addon default-storageclass should already be in state true
I0918 00:59:31.379089   52240 out.go:177]     ‚ñ™ Utilisation de l'image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0918 00:59:31.375078   52240 host.go:66] Checking if "minikube" exists ...
I0918 00:59:31.365070   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:59:31.374431   52240 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0918 00:59:31.374527   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0918 00:59:31.364972   52240 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0918 00:59:31.380328   52240 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0918 00:59:31.382849   52240 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0918 00:59:31.387335   52240 out.go:177]     ‚ñ™ Utilisation de l'image registry.k8s.io/metrics-server/metrics-server:v0.6.4
I0918 00:59:31.387524   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0918 00:59:31.393198   52240 addons.go:423] installing /etc/kubernetes/addons/metrics-apiservice.yaml
I0918 00:59:31.393231   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-apiservice.yaml (424 bytes)
I0918 00:59:31.387641   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0918 00:59:31.393500   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:59:31.393713   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:59:31.393774   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:59:31.756841   52240 api_server.go:52] waiting for apiserver process to appear ...
I0918 00:59:31.757409   52240 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0918 00:59:31.758595   52240 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0918 00:59:31.758606   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0918 00:59:31.758759   52240 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0918 00:59:31.760164   52240 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57551 SSHKeyPath:/Users/jam/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:59:31.768611   52240 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57551 SSHKeyPath:/Users/jam/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:59:31.772296   52240 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57551 SSHKeyPath:/Users/jam/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:59:31.776037   52240 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57551 SSHKeyPath:/Users/jam/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:59:31.813056   52240 api_server.go:72] duration metric: took 787.171173ms to wait for apiserver process to appear ...
I0918 00:59:31.813073   52240 api_server.go:88] waiting for apiserver healthz status ...
I0918 00:59:31.813114   52240 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57550/healthz ...
I0918 00:59:31.831019   52240 api_server.go:279] https://127.0.0.1:57550/healthz returned 200:
ok
I0918 00:59:31.836239   52240 api_server.go:141] control plane version: v1.28.3
I0918 00:59:31.836250   52240 api_server.go:131] duration metric: took 23.173366ms to wait for apiserver health ...
I0918 00:59:31.836255   52240 system_pods.go:43] waiting for kube-system pods to appear ...
I0918 00:59:31.858083   52240 system_pods.go:59] 9 kube-system pods found
I0918 00:59:31.858098   52240 system_pods.go:61] "coredns-5dd5756b68-2sqx9" [cc684e72-bf1a-4286-a82e-9a8e7f8f5e81] Running
I0918 00:59:31.858104   52240 system_pods.go:61] "etcd-minikube" [324f41ac-5401-4bd3-b114-6bc033901b0a] Running
I0918 00:59:31.858110   52240 system_pods.go:61] "kube-apiserver-minikube" [9a97f7a8-8dce-46b0-892d-69c46521a95f] Running
I0918 00:59:31.858116   52240 system_pods.go:61] "kube-controller-manager-minikube" [6f79b6f3-bdc5-490f-bdcf-da5d78e2b4f2] Running
I0918 00:59:31.858121   52240 system_pods.go:61] "kube-proxy-48xsd" [31824f72-597d-4319-8f37-075475c4edfe] Running
I0918 00:59:31.858126   52240 system_pods.go:61] "kube-scheduler-minikube" [6db0797c-020c-4b63-87f2-ae9a93a07084] Running
I0918 00:59:31.858131   52240 system_pods.go:61] "kubevirt-install-manager" [6b4ec9e3-7c91-4b92-b911-ebcb1331c6da] Running
I0918 00:59:31.858137   52240 system_pods.go:61] "metrics-server-7c66d45ddc-ffjlk" [1b350d94-9de9-4cf7-a758-e66fd169fedb] Running
I0918 00:59:31.858146   52240 system_pods.go:61] "storage-provisioner" [688c3951-205d-4398-a857-a0822361a6c3] Running
I0918 00:59:31.858159   52240 system_pods.go:74] duration metric: took 21.892825ms to wait for pod list to return data ...
I0918 00:59:31.858169   52240 kubeadm.go:581] duration metric: took 832.302754ms to wait for : map[apiserver:true system_pods:true] ...
I0918 00:59:31.858180   52240 node_conditions.go:102] verifying NodePressure condition ...
I0918 00:59:31.868935   52240 node_conditions.go:122] node storage ephemeral capacity is 61252420Ki
I0918 00:59:31.868949   52240 node_conditions.go:123] node cpu capacity is 4
I0918 00:59:31.868972   52240 node_conditions.go:105] duration metric: took 10.786801ms to run NodePressure ...
I0918 00:59:31.868984   52240 start.go:228] waiting for startup goroutines ...
I0918 00:59:31.936708   52240 addons.go:423] installing /etc/kubernetes/addons/metrics-server-deployment.yaml
I0918 00:59:31.936725   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-deployment.yaml (1907 bytes)
I0918 00:59:31.963632   52240 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/pod.yaml
I0918 00:59:31.983643   52240 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0918 00:59:31.983659   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0918 00:59:31.989167   52240 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0918 00:59:31.996307   52240 addons.go:423] installing /etc/kubernetes/addons/metrics-server-rbac.yaml
I0918 00:59:31.996318   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-rbac.yaml (2175 bytes)
I0918 00:59:31.999000   52240 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57551 SSHKeyPath:/Users/jam/.minikube/machines/minikube/id_rsa Username:docker}
I0918 00:59:32.033061   52240 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0918 00:59:32.033075   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0918 00:59:32.064471   52240 addons.go:423] installing /etc/kubernetes/addons/metrics-server-service.yaml
I0918 00:59:32.064491   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/metrics-server-service.yaml (446 bytes)
I0918 00:59:32.127349   52240 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0918 00:59:32.127362   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0918 00:59:32.144940   52240 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml
I0918 00:59:32.216727   52240 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0918 00:59:32.216738   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0918 00:59:32.338915   52240 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0918 00:59:32.346323   52240 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I0918 00:59:32.346337   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0918 00:59:32.560194   52240 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0918 00:59:32.560240   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0918 00:59:32.660741   52240 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0918 00:59:32.660754   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0918 00:59:32.763581   52240 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0918 00:59:32.763598   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0918 00:59:32.848824   52240 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0918 00:59:32.848843   52240 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0918 00:59:32.876927   52240 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0918 00:59:34.315838   52240 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/pod.yaml: (2.352257882s)
I0918 00:59:34.547964   52240 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.558846161s)
I0918 00:59:34.623784   52240 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/metrics-apiservice.yaml -f /etc/kubernetes/addons/metrics-server-deployment.yaml -f /etc/kubernetes/addons/metrics-server-rbac.yaml -f /etc/kubernetes/addons/metrics-server-service.yaml: (2.478915182s)
I0918 00:59:34.623819   52240 addons.go:467] Verifying addon metrics-server=true in "minikube"
I0918 00:59:34.623880   52240 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.285022844s)
I0918 00:59:35.111069   52240 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (2.234164197s)
I0918 00:59:35.117319   52240 out.go:177] üí°  Certaines fonctionnalit√©s du tableau de bord n√©cessitent le module metrics-server. Pour activer toutes les fonctionnalit√©s, veuillez ex√©cuter¬†:

	minikube addons enable metrics-server	


I0918 00:59:35.120969   52240 out.go:177] üåü  Modules activ√©s: kubevirt, storage-provisioner, metrics-server, default-storageclass, dashboard
I0918 00:59:35.124409   52240 addons.go:502] enable addons completed in 4.126804614s: enabled=[kubevirt storage-provisioner metrics-server default-storageclass dashboard]
I0918 00:59:35.124447   52240 start.go:233] waiting for cluster config update ...
I0918 00:59:35.124462   52240 start.go:242] writing updated cluster config ...
I0918 00:59:35.127149   52240 ssh_runner.go:195] Run: rm -f paused
I0918 00:59:35.179217   52240 start.go:600] kubectl: 1.22.5, cluster: 1.28.3 (minor skew: 6)
I0918 00:59:35.183824   52240 out.go:177] 
W0918 00:59:35.188198   52240 out.go:239] ‚ùó  /usr/local/bin/kubectl est la version 1.22.5, qui peut comporter des incompatibilit√©s avec Kubernetes 1.28.3.
I0918 00:59:35.192528   52240 out.go:177]     ‚ñ™ Vous voulez kubectl v1.28.3¬†? Essayez 'minikube kubectl -- get pods -A'
I0918 00:59:35.200053   52240 out.go:177] üèÑ  Termin√© ! kubectl est maintenant configur√© pour utiliser "minikube" cluster et espace de noms "default" par d√©faut.

* 
* ==> Docker <==
* Sep 18 07:44:35 minikube cri-dockerd[1138]: time="2024-09-18T07:44:35Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Sep 18 07:44:36 minikube cri-dockerd[1138]: time="2024-09-18T07:44:36Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Sep 18 07:44:38 minikube cri-dockerd[1138]: time="2024-09-18T07:44:38Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Sep 18 07:44:39 minikube cri-dockerd[1138]: time="2024-09-18T07:44:39Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Sep 18 07:44:40 minikube cri-dockerd[1138]: time="2024-09-18T07:44:40Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Sep 18 07:44:42 minikube cri-dockerd[1138]: time="2024-09-18T07:44:42Z" level=info msg="Stop pulling image openlab29/k8bis:latest: Status: Image is up to date for openlab29/k8bis:latest"
Sep 18 07:46:20 minikube dockerd[887]: time="2024-09-18T07:46:20.879353100Z" level=info msg="ignoring event" container=d333225544b1aa94fcdea55875bec76c356875890dca7f1aac57b13dc394378f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:46:20 minikube dockerd[887]: time="2024-09-18T07:46:20.957307700Z" level=info msg="ignoring event" container=9af076ba4952ad54d578a014f681a6e797d84ee205fc51649d4acf4884df69e1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:46:21 minikube dockerd[887]: time="2024-09-18T07:46:21.570653500Z" level=info msg="ignoring event" container=a7a09031611e425b9067c70ee6dd33bfcfc7934ac0b1d042a64275d5f5ab8793 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:46:21 minikube dockerd[887]: time="2024-09-18T07:46:21.578218100Z" level=info msg="ignoring event" container=44b0ef43507e38d1eeb9637521da57461848f33acf05e483e3bcb11486eb84a8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:46:22 minikube cri-dockerd[1138]: time="2024-09-18T07:46:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cfa07b79268739a429248d5d7788badcee89582470953212f2bfa146152b123e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 18 07:46:22 minikube cri-dockerd[1138]: time="2024-09-18T07:46:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d8844c9ce6b810f8c13327942a5dd1b764c82ac3e89603a7b229de765f33531a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 18 07:46:23 minikube cri-dockerd[1138]: time="2024-09-18T07:46:23Z" level=info msg="Stop pulling image openlab29/asso1:latest: Status: Image is up to date for openlab29/asso1:latest"
Sep 18 07:46:25 minikube cri-dockerd[1138]: time="2024-09-18T07:46:25Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Sep 18 07:46:26 minikube dockerd[887]: time="2024-09-18T07:46:26.868372700Z" level=info msg="ignoring event" container=a3db335b711ac0c8eb399419d93e2bf63292f3daa5afa30c080cb01ec5061020 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:46:27 minikube dockerd[887]: time="2024-09-18T07:46:27.249838400Z" level=info msg="ignoring event" container=87f994424e9f97068d66450ad5d64ece4dd471b551f199f6fa84be20c684bd84 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:46:27 minikube cri-dockerd[1138]: time="2024-09-18T07:46:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4c1507f92fc9bcfc9398b4e46eb2c97cd7276a07cb95a97afa090182b450a2d1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 18 07:46:29 minikube cri-dockerd[1138]: time="2024-09-18T07:46:29Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Sep 18 07:46:30 minikube dockerd[887]: time="2024-09-18T07:46:30.159630300Z" level=info msg="ignoring event" container=ffdd90c1dc5cf32936564d04dc90b234c2ddd58f5ec96d886cd85c75bff521fc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:46:30 minikube dockerd[887]: time="2024-09-18T07:46:30.317618600Z" level=info msg="ignoring event" container=db19de9d806ba917f4f9a37ae6e113bb5d0652a726299ef209aa861bf263d565 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:46:30 minikube cri-dockerd[1138]: time="2024-09-18T07:46:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9f79baca8066b253e1c48335492d44ac93fc14542ec2b3ec4e8d46adcde795ac/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 18 07:46:31 minikube cri-dockerd[1138]: time="2024-09-18T07:46:31Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Sep 18 07:46:33 minikube dockerd[887]: time="2024-09-18T07:46:33.267866600Z" level=info msg="ignoring event" container=21d18a057104784390d97c60ea56666fc373f1f3a0ef03c7f996e02f95a841bd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:46:33 minikube dockerd[887]: time="2024-09-18T07:46:33.439104700Z" level=info msg="ignoring event" container=ada7336cc7787ca6962aa9134c81aaf3909b4826134ad68bbda3edff093564ed module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:46:34 minikube dockerd[887]: time="2024-09-18T07:46:34.354173000Z" level=info msg="ignoring event" container=af8565dfb00e3cde86b0837223f6c3f37a4fbd714d614e095547dfd673850fdd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:46:36 minikube cri-dockerd[1138]: time="2024-09-18T07:46:36Z" level=info msg="Stop pulling image openlab29/asso1:latest: Status: Image is up to date for openlab29/asso1:latest"
Sep 18 07:46:40 minikube dockerd[887]: time="2024-09-18T07:46:40.628038300Z" level=info msg="ignoring event" container=e0ed3f804823298c1272f6d9f311dee2d7dffac3bb983e5ea8421d67d8392124 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:46:53 minikube cri-dockerd[1138]: time="2024-09-18T07:46:53Z" level=info msg="Stop pulling image openlab29/asso1:latest: Status: Image is up to date for openlab29/asso1:latest"
Sep 18 07:55:11 minikube dockerd[887]: time="2024-09-18T07:55:11.996561200Z" level=info msg="ignoring event" container=e089f6b83ada82e8ddcfa93848a4f8196dc9432310a07ee3462fc2e342d6caff module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:55:37 minikube cri-dockerd[1138]: time="2024-09-18T07:55:37Z" level=info msg="Stop pulling image openlab29/asso1:latest: Status: Image is up to date for openlab29/asso1:latest"
Sep 18 07:55:47 minikube dockerd[887]: time="2024-09-18T07:55:47.440502000Z" level=info msg="ignoring event" container=2eb054a25aaeb39b14ca4ff8cba2f0173164b47d0209dafaafc1807e184a59cd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:56:32 minikube cri-dockerd[1138]: time="2024-09-18T07:56:32Z" level=info msg="Stop pulling image openlab29/asso1:latest: Status: Image is up to date for openlab29/asso1:latest"
Sep 18 07:58:17 minikube dockerd[887]: time="2024-09-18T07:58:17.681235200Z" level=info msg="ignoring event" container=fd0b9454d7b33aeb27cbffe644364620109beffdfeeda105e2f947d843ada571 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:58:17 minikube dockerd[887]: time="2024-09-18T07:58:17.931733700Z" level=info msg="ignoring event" container=f08768e0471fa5d418a80955169f140d24d779869597deafd883ae220f3dd306 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:58:18 minikube dockerd[887]: time="2024-09-18T07:58:18.095395600Z" level=info msg="ignoring event" container=1945546138bcd4a5839d353af1730eddc0b5a301500e5daf274717dcd1ed4900 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:58:18 minikube dockerd[887]: time="2024-09-18T07:58:18.213379400Z" level=info msg="ignoring event" container=b3254be0fefed2c23170dd8d09d71f8b82f26323b7515084a37578677ee63892 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:58:18 minikube dockerd[887]: time="2024-09-18T07:58:18.297808100Z" level=info msg="ignoring event" container=f4c62f442d6673caa929be5e973fd04c9e323cb9e8c6bb6f909d626a05946353 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:58:18 minikube dockerd[887]: time="2024-09-18T07:58:18.396852000Z" level=info msg="ignoring event" container=1397b9d60ca38a06fca133e65ece64bcb26aa752d52452c2f6b06ae3da7eee53 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:58:18 minikube dockerd[887]: time="2024-09-18T07:58:18.518868200Z" level=info msg="ignoring event" container=3811e4c19e728c6a36e02f3fef3e9f80f0f5bdbf55dca8f9d7d514c7e6c7d074 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:58:18 minikube cri-dockerd[1138]: time="2024-09-18T07:58:18Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-86c459c477-4h9ft_default\": unexpected command output nsenter: cannot open /proc/874335/ns/net: No such file or directory\n with error: exit status 1"
Sep 18 07:58:18 minikube dockerd[887]: time="2024-09-18T07:58:18.726664000Z" level=info msg="ignoring event" container=cfa07b79268739a429248d5d7788badcee89582470953212f2bfa146152b123e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:58:19 minikube dockerd[887]: time="2024-09-18T07:58:19.332797900Z" level=info msg="ignoring event" container=4c1507f92fc9bcfc9398b4e46eb2c97cd7276a07cb95a97afa090182b450a2d1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:58:19 minikube dockerd[887]: time="2024-09-18T07:58:19.408025800Z" level=info msg="ignoring event" container=d8844c9ce6b810f8c13327942a5dd1b764c82ac3e89603a7b229de765f33531a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:58:19 minikube dockerd[887]: time="2024-09-18T07:58:19.533883900Z" level=info msg="ignoring event" container=3a09bc1814208e44b7d8e5e4bcc14c1e6cfc42b6d95273c4922c8e724fd09f87 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:58:19 minikube dockerd[887]: time="2024-09-18T07:58:19.799165300Z" level=info msg="ignoring event" container=c6b7857828dff5183229c0c0dafb8e43eae8b155c0e06e6271dc35c54f185ba2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:58:19 minikube dockerd[887]: time="2024-09-18T07:58:19.827841200Z" level=info msg="ignoring event" container=d1c76d1e12881c3d88cb28fc10c9af3cd53254ddf5c1c461d1c702e2190b4c95 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:58:19 minikube dockerd[887]: time="2024-09-18T07:58:19.829515400Z" level=info msg="ignoring event" container=9f79baca8066b253e1c48335492d44ac93fc14542ec2b3ec4e8d46adcde795ac module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:58:20 minikube cri-dockerd[1138]: time="2024-09-18T07:58:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/45951f067103a279923d94674bd361efb4215765ddb2fbead98e6df2e837c6d9/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 18 07:58:21 minikube cri-dockerd[1138]: time="2024-09-18T07:58:21Z" level=info msg="Stop pulling image openlab29/k8bis:latest: Status: Image is up to date for openlab29/k8bis:latest"
Sep 18 07:58:35 minikube cri-dockerd[1138]: time="2024-09-18T07:58:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fe8b5d0e96844fceefb64464e6b74e8f6ea3177140d43cbcd4cb4b24b197d966/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 18 07:58:35 minikube cri-dockerd[1138]: time="2024-09-18T07:58:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e5a6f3756e5725845c76c6ec20efdfde5ce60877e10c8c8476149bce4f62fa24/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 18 07:58:35 minikube cri-dockerd[1138]: time="2024-09-18T07:58:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/57ce9f6cf3bbe67638727ddb06f7bf709f191f303a6da8a154d073de19475dcc/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 18 07:58:35 minikube cri-dockerd[1138]: time="2024-09-18T07:58:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bd5a91618ecc0edd4b081e15944e538e324ae503abd8c4d6e53244eb6abd71dd/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 18 07:58:37 minikube cri-dockerd[1138]: time="2024-09-18T07:58:37Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Sep 18 07:58:38 minikube cri-dockerd[1138]: time="2024-09-18T07:58:38Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Sep 18 07:58:39 minikube cri-dockerd[1138]: time="2024-09-18T07:58:39Z" level=info msg="Stop pulling image openlab29/asso1:latest: Status: Image is up to date for openlab29/asso1:latest"
Sep 18 07:58:41 minikube cri-dockerd[1138]: time="2024-09-18T07:58:41Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Sep 18 07:58:53 minikube dockerd[887]: time="2024-09-18T07:58:53.075555000Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=b67e16d3fb17f42276fb27854f6882b756cb89a724a652e32f16be3d752284de
Sep 18 07:58:53 minikube dockerd[887]: time="2024-09-18T07:58:53.123162700Z" level=info msg="ignoring event" container=b67e16d3fb17f42276fb27854f6882b756cb89a724a652e32f16be3d752284de module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 18 07:58:53 minikube dockerd[887]: time="2024-09-18T07:58:53.248677800Z" level=info msg="ignoring event" container=45951f067103a279923d94674bd361efb4215765ddb2fbead98e6df2e837c6d9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                     CREATED              STATE               NAME                        ATTEMPT             POD ID              POD
057bd56bd7c80       nginx@sha256:04ba374043ccd2fc5c593885c0eacddebabd5ca375f9323666f28dfd5a9710e3             57 seconds ago       Running             nginx                       0                   bd5a91618ecc0       nginx-86c459c477-lpxzt
597d60b241833       openlab29/asso1@sha256:726bd0c4d6a912d9eeaafd695f80ab95824dff11756e8e2ee06189490abb1669   59 seconds ago       Running             asso1                       0                   57ce9f6cf3bbe       asso1-c47fd9945-plmlg
ec934566c039b       nginx@sha256:04ba374043ccd2fc5c593885c0eacddebabd5ca375f9323666f28dfd5a9710e3             About a minute ago   Running             nginx                       0                   fe8b5d0e96844       nginx-86c459c477-4lj7f
c37bea38e88bc       nginx@sha256:04ba374043ccd2fc5c593885c0eacddebabd5ca375f9323666f28dfd5a9710e3             About a minute ago   Running             nginx                       0                   e5a6f3756e572       nginx-86c459c477-xvsfv
0667052406bae       07655ddf2eebe                                                                             9 hours ago          Running             kubernetes-dashboard        12                  d722a83a3121a       kubernetes-dashboard-8694d4445c-z6fkk
581f863026b2e       6e38f40d628db                                                                             9 hours ago          Running             storage-provisioner         15                  14cf63b45b326       storage-provisioner
b4723d1c18d36       e23f7ca36333c                                                                             9 hours ago          Running             kube-flannel                1                   9c5e94a50a0d0       kube-flannel-ds-4pd86
bf8f3a6ea4613       e23f7ca36333c                                                                             9 hours ago          Exited              install-cni                 0                   9c5e94a50a0d0       kube-flannel-ds-4pd86
28c971042115b       7902ba679ba80                                                                             9 hours ago          Running             kubevirt-provisioner        1                   ad7fb49a82ad0       kubevirt-install-manager
1d7b6a438317c       a608c686bac93                                                                             9 hours ago          Running             metrics-server              8                   fac55b2e79a10       metrics-server-7c66d45ddc-ffjlk
9370e8a2a9710       ead0a4a53df89                                                                             9 hours ago          Running             coredns                     7                   2cb3c0f0b685c       coredns-5dd5756b68-2sqx9
dac51631bdd91       07655ddf2eebe                                                                             9 hours ago          Exited              kubernetes-dashboard        11                  d722a83a3121a       kubernetes-dashboard-8694d4445c-z6fkk
cbfc3a0d86104       115053965e86b                                                                             9 hours ago          Running             dashboard-metrics-scraper   7                   127746f4ce2ad       dashboard-metrics-scraper-7fd5cb4ddc-qsvgl
9dbc1a5417f86       a55d1bad692b7                                                                             9 hours ago          Exited              install-cni-plugin          1                   9c5e94a50a0d0       kube-flannel-ds-4pd86
b212bd94b61eb       bfc896cf80fba                                                                             9 hours ago          Running             kube-proxy                  7                   b2431297166d3       kube-proxy-48xsd
14d7245bce800       6e38f40d628db                                                                             9 hours ago          Exited              storage-provisioner         14                  14cf63b45b326       storage-provisioner
5381e7606147e       6d1b4fd1b182d                                                                             9 hours ago          Running             kube-scheduler              7                   8689243ac7bf3       kube-scheduler-minikube
5f59ef5e65d40       10baa1ca17068                                                                             9 hours ago          Running             kube-controller-manager     7                   cc8ee08a46bb4       kube-controller-manager-minikube
c5ab0f38ceeb0       5374347291230                                                                             9 hours ago          Running             kube-apiserver              7                   db1ba5ecdf3a0       kube-apiserver-minikube
aa19383d49005       73deb9a3f7025                                                                             9 hours ago          Running             etcd                        7                   a5e1c0bb74322       etcd-minikube
e7848b2348376       bitnami/kubectl@sha256:195f5a7a40cfb06e308701ae850abfa436d23baf9d39c0282298e540c9d07863   17 hours ago         Exited              kubevirt-provisioner        0                   8af58c4646d7d       kubevirt-install-manager
33a4589f3572d       e23f7ca36333c                                                                             17 hours ago         Exited              kube-flannel                0                   84c5cbba2e677       kube-flannel-ds-4pd86
7e1e9934d2c4a       ead0a4a53df89                                                                             17 hours ago         Exited              coredns                     6                   bf2d1805e1f38       coredns-5dd5756b68-2sqx9
bb2deb81b0e6f       a608c686bac93                                                                             17 hours ago         Exited              metrics-server              7                   77f04ce3ebd4e       metrics-server-7c66d45ddc-ffjlk
4a0eaee862470       115053965e86b                                                                             17 hours ago         Exited              dashboard-metrics-scraper   6                   00dd7a7f96b02       dashboard-metrics-scraper-7fd5cb4ddc-qsvgl
756abc953a483       bfc896cf80fba                                                                             17 hours ago         Exited              kube-proxy                  6                   8087e7a1fe96a       kube-proxy-48xsd
e8bd370ad18e8       5374347291230                                                                             17 hours ago         Exited              kube-apiserver              6                   e09eb81e9b425       kube-apiserver-minikube
ccedc038ab1aa       73deb9a3f7025                                                                             17 hours ago         Exited              etcd                        6                   80d2544fc889d       etcd-minikube
cc443522d7d8b       6d1b4fd1b182d                                                                             17 hours ago         Exited              kube-scheduler              6                   5bcae6a219a7a       kube-scheduler-minikube
a42cabb84c384       10baa1ca17068                                                                             17 hours ago         Exited              kube-controller-manager     6                   d1c03cc2a9d37       kube-controller-manager-minikube

* 
* ==> coredns [7e1e9934d2c4] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 8846d9ca81164c00fa03e78dfcf1a6846552cc49335bc010218794b8cfaf537759aa4b596e7dc20c0f618e8eb07603c0139662b99dfa3de45b176fbe7fb57ce1
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:50943 - 46580 "HINFO IN 76136444901116485.292306896344717969. udp 54 false 512" NXDOMAIN qr,rd,ra 54 0.2831946s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [9370e8a2a971] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 8846d9ca81164c00fa03e78dfcf1a6846552cc49335bc010218794b8cfaf537759aa4b596e7dc20c0f618e8eb07603c0139662b99dfa3de45b176fbe7fb57ce1
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:39782 - 54834 "HINFO IN 7708897000214564707.2813155518166378169. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.0760064s
[INFO] 10.244.0.66:58994 - 47535 "A IN nginx.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.0024649s
[INFO] 10.244.0.80:43989 - 53590 "A IN nginx.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.0015185s
[INFO] 10.244.0.89:40770 - 39815 "A IN nginx.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.0003145s
[INFO] 10.244.0.89:52725 - 15800 "A IN nginx.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.001463s
[INFO] 10.244.0.98:40298 - 12455 "A IN nginx.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.0018518s
[INFO] 10.244.0.100:37600 - 61815 "A IN nginx.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.0015248s
[INFO] 10.244.0.100:34707 - 12145 "A IN nginx.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.0003883s
[INFO] 10.244.0.100:36742 - 13439 "A IN nginx.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000541s
[INFO] 10.244.0.100:55692 - 1429 "A IN nginx.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.0002437s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_09_09T11_36_09_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"12:56:15:b8:b2:51"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 192.168.49.2
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 09 Sep 2024 09:36:06 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 18 Sep 2024 07:59:29 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Tue, 17 Sep 2024 22:59:10 +0000   Tue, 17 Sep 2024 22:59:10 +0000   FlannelIsUp                  Flannel is running on this node
  MemoryPressure       False   Wed, 18 Sep 2024 07:57:02 +0000   Mon, 09 Sep 2024 09:36:04 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Wed, 18 Sep 2024 07:57:02 +0000   Mon, 09 Sep 2024 09:36:04 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Wed, 18 Sep 2024 07:57:02 +0000   Mon, 09 Sep 2024 09:36:04 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Wed, 18 Sep 2024 07:57:02 +0000   Mon, 09 Sep 2024 09:36:06 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  61252420Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             5054836Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  61252420Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             5054836Ki
  pods:               110
System Info:
  Machine ID:                 10fec8ec458a4cb7b7f4f29cbee00c7b
  System UUID:                7ec3387a-6493-4f20-86e3-f66d996e516a
  Boot ID:                    82117cd7-24dd-42ca-98ae-a86e8c1e80b5
  Kernel Version:             5.10.76-linuxkit
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (16 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     asso1-c47fd9945-plmlg                         500m (12%!)(MISSING)    500m (12%!)(MISSING)  128Mi (2%!)(MISSING)       128Mi (2%!)(MISSING)     65s
  default                     nginx-86c459c477-4lj7f                        500m (12%!)(MISSING)    500m (12%!)(MISSING)  128Mi (2%!)(MISSING)       128Mi (2%!)(MISSING)     65s
  default                     nginx-86c459c477-lpxzt                        500m (12%!)(MISSING)    500m (12%!)(MISSING)  128Mi (2%!)(MISSING)       128Mi (2%!)(MISSING)     65s
  default                     nginx-86c459c477-xvsfv                        500m (12%!)(MISSING)    500m (12%!)(MISSING)  128Mi (2%!)(MISSING)       128Mi (2%!)(MISSING)     65s
  kube-flannel                kube-flannel-ds-4pd86                         100m (2%!)(MISSING)     0 (0%!)(MISSING)      50Mi (1%!)(MISSING)        0 (0%!)(MISSING)         17h
  kube-system                 coredns-5dd5756b68-2sqx9                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (3%!)(MISSING)     8d
  kube-system                 etcd-minikube                                 100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         8d
  kube-system                 kube-apiserver-minikube                       250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
  kube-system                 kube-controller-manager-minikube              200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
  kube-system                 kube-proxy-48xsd                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
  kube-system                 kube-scheduler-minikube                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
  kube-system                 kubevirt-install-manager                      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17h
  kube-system                 metrics-server-7c66d45ddc-ffjlk               100m (2%!)(MISSING)     0 (0%!)(MISSING)      200Mi (4%!)(MISSING)       0 (0%!)(MISSING)         8d
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-qsvgl    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-z6fkk         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                2950m (73%!)(MISSING)  2 (50%!)(MISSING)
  memory             932Mi (18%!)(MISSING)  682Mi (13%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [Sep17 11:44] ERROR: earlyprintk= earlyser already used
[  +0.000000] ERROR: earlyprintk= earlyser already used
[  +0.000000] ACPI BIOS Warning (bug): Incorrect checksum in table [DSDT] - 0x7E, should be 0xDB (20200925/tbprint-173)
[  +0.000000]  #2
[  +0.068010]  #3
[  +2.082716] Hangcheck: starting hangcheck timer 0.9.1 (tick is 180 seconds, margin is 60 seconds).
[  +0.017560] the cryptoloop driver has been deprecated and will be removed in in Linux 5.16
[  +0.030312] ACPI Error: Could not enable RealTimeClock event (20200925/evxfevnt-182)
[  +0.003326] ACPI Warning: Could not enable fixed event - RealTimeClock (4) (20200925/evxface-618)
[Sep17 11:45] grpcfuse: loading out-of-tree module taints kernel.
[Sep17 14:31] clocksource: timekeeping watchdog on CPU1: Marking clocksource 'tsc' as unstable because the skew is too large:
[  +0.004482] clocksource:                       'hpet' wd_now: 686bcd8e wd_last: 66ec34c2 mask: ffffffff
[  +0.004755] clocksource:                       'tsc' cs_now: 17e8cb64da7c cs_last: 17e75e7addba mask: ffffffffffffffff
[  +0.138078] TSC found unstable after boot, most likely due to broken BIOS. Use 'tsc=unstable'.
[  +0.008965] clocksource: Checking clocksource tsc synchronization from CPU 2.
[Sep17 14:43] hrtimer: interrupt took 102578300 ns
[Sep17 23:49] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.

* 
* ==> etcd [aa19383d4900] <==
* {"level":"warn","ts":"2024-09-18T06:45:59.300272Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.5084ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031965687690090 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/ranges/serviceips\" mod_revision:104839 > success:<request_put:<key:\"/registry/ranges/serviceips\" value_size:121926 >> failure:<request_range:<key:\"/registry/ranges/serviceips\" > >>","response":"size:18"}
{"level":"info","ts":"2024-09-18T06:45:59.30091Z","caller":"traceutil/trace.go:171","msg":"trace[1839434416] transaction","detail":"{read_only:false; response_revision:105989; number_of_response:1; }","duration":"189.3176ms","start":"2024-09-18T06:45:59.111512Z","end":"2024-09-18T06:45:59.300876Z","steps":["trace[1839434416] 'process raft request'  (duration: 77.9908ms)","trace[1839434416] 'compare'  (duration: 16.0523ms)"],"step_count":2}
{"level":"info","ts":"2024-09-18T06:45:59.30094Z","caller":"traceutil/trace.go:171","msg":"trace[246286982] linearizableReadLoop","detail":"{readStateIndex:132677; appliedIndex:132675; }","duration":"103.0455ms","start":"2024-09-18T06:45:59.197851Z","end":"2024-09-18T06:45:59.300898Z","steps":["trace[246286982] 'read index received'  (duration: 3.7174ms)","trace[246286982] 'applied index is now lower than readState.Index'  (duration: 99.2939ms)"],"step_count":2}
{"level":"info","ts":"2024-09-18T06:45:59.302197Z","caller":"traceutil/trace.go:171","msg":"trace[1812121827] transaction","detail":"{read_only:false; response_revision:105990; number_of_response:1; }","duration":"109.7015ms","start":"2024-09-18T06:45:59.192446Z","end":"2024-09-18T06:45:59.302144Z","steps":["trace[1812121827] 'process raft request'  (duration: 108.1344ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-18T06:45:59.302668Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.4963ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/default/default\" ","response":"range_response_count:1 size:172"}
{"level":"info","ts":"2024-09-18T06:45:59.302805Z","caller":"traceutil/trace.go:171","msg":"trace[771446906] range","detail":"{range_begin:/registry/serviceaccounts/default/default; range_end:; response_count:1; response_revision:105990; }","duration":"105.6748ms","start":"2024-09-18T06:45:59.197091Z","end":"2024-09-18T06:45:59.302762Z","steps":["trace[771446906] 'agreement among raft nodes before linearized reading'  (duration: 105.1628ms)"],"step_count":1}
{"level":"info","ts":"2024-09-18T06:46:38.486275Z","caller":"traceutil/trace.go:171","msg":"trace[40808948] linearizableReadLoop","detail":"{readStateIndex:132823; appliedIndex:132823; }","duration":"130.0602ms","start":"2024-09-18T06:46:38.356108Z","end":"2024-09-18T06:46:38.486214Z","steps":["trace[40808948] 'read index received'  (duration: 129.9817ms)","trace[40808948] 'applied index is now lower than readState.Index'  (duration: 36.9¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-09-18T06:46:38.543154Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"188.0331ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/nginx-7b5765f844-lwlbc\" ","response":"range_response_count:1 size:2652"}
{"level":"info","ts":"2024-09-18T06:46:38.544061Z","caller":"traceutil/trace.go:171","msg":"trace[690850284] range","detail":"{range_begin:/registry/pods/default/nginx-7b5765f844-lwlbc; range_end:; response_count:1; response_revision:106126; }","duration":"209.8445ms","start":"2024-09-18T06:46:38.332344Z","end":"2024-09-18T06:46:38.544Z","steps":["trace[690850284] 'agreement among raft nodes before linearized reading'  (duration: 152.27ms)","trace[690850284] 'range keys from in-memory index tree'  (duration: 56.4851ms)"],"step_count":2}
{"level":"info","ts":"2024-09-18T06:49:38.881451Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":105840}
{"level":"info","ts":"2024-09-18T06:49:38.883193Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":105840,"took":"1.2035ms","hash":1813998093}
{"level":"info","ts":"2024-09-18T06:49:38.883342Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1813998093,"revision":105840,"compact-revision":105600}
{"level":"info","ts":"2024-09-18T06:54:38.571344Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":106302}
{"level":"info","ts":"2024-09-18T06:54:38.578975Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":106302,"took":"6.8753ms","hash":3487785133}
{"level":"info","ts":"2024-09-18T06:54:38.579084Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3487785133,"revision":106302,"compact-revision":105840}
{"level":"info","ts":"2024-09-18T06:59:38.226015Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":106541}
{"level":"info","ts":"2024-09-18T06:59:38.227913Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":106541,"took":"1.1778ms","hash":1974857037}
{"level":"info","ts":"2024-09-18T06:59:38.228026Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1974857037,"revision":106541,"compact-revision":106302}
{"level":"info","ts":"2024-09-18T07:04:37.875134Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":106779}
{"level":"info","ts":"2024-09-18T07:04:37.876126Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":106779,"took":"637¬µs","hash":4108949642}
{"level":"info","ts":"2024-09-18T07:04:37.876208Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4108949642,"revision":106779,"compact-revision":106541}
{"level":"info","ts":"2024-09-18T07:09:37.523709Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":107017}
{"level":"info","ts":"2024-09-18T07:09:37.525344Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":107017,"took":"1.173ms","hash":620379429}
{"level":"info","ts":"2024-09-18T07:09:37.525448Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":620379429,"revision":107017,"compact-revision":106779}
{"level":"info","ts":"2024-09-18T07:14:37.17202Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":107256}
{"level":"info","ts":"2024-09-18T07:14:37.174288Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":107256,"took":"823.5¬µs","hash":3291908751}
{"level":"info","ts":"2024-09-18T07:14:37.174381Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3291908751,"revision":107256,"compact-revision":107017}
{"level":"info","ts":"2024-09-18T07:19:36.820917Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":107496}
{"level":"info","ts":"2024-09-18T07:19:36.822957Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":107496,"took":"1.0285ms","hash":3572079237}
{"level":"info","ts":"2024-09-18T07:19:36.823918Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3572079237,"revision":107496,"compact-revision":107256}
{"level":"info","ts":"2024-09-18T07:24:36.43385Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":107735}
{"level":"info","ts":"2024-09-18T07:24:36.434912Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":107735,"took":"701.4¬µs","hash":689281343}
{"level":"info","ts":"2024-09-18T07:24:36.435062Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":689281343,"revision":107735,"compact-revision":107496}
{"level":"info","ts":"2024-09-18T07:26:51.101397Z","caller":"traceutil/trace.go:171","msg":"trace[1636309292] transaction","detail":"{read_only:false; response_revision:108116; number_of_response:1; }","duration":"121.9241ms","start":"2024-09-18T07:26:50.979435Z","end":"2024-09-18T07:26:51.101359Z","steps":["trace[1636309292] 'process raft request'  (duration: 121.4098ms)"],"step_count":1}
{"level":"info","ts":"2024-09-18T07:27:19.290634Z","caller":"traceutil/trace.go:171","msg":"trace[923679370] transaction","detail":"{read_only:false; response_revision:108166; number_of_response:1; }","duration":"102.197ms","start":"2024-09-18T07:27:19.188382Z","end":"2024-09-18T07:27:19.290573Z","steps":["trace[923679370] 'process raft request'  (duration: 94.8866ms)"],"step_count":1}
{"level":"info","ts":"2024-09-18T07:29:36.078325Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":107974}
{"level":"info","ts":"2024-09-18T07:29:36.079469Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":107974,"took":"720.6¬µs","hash":2714715282}
{"level":"info","ts":"2024-09-18T07:29:36.079944Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2714715282,"revision":107974,"compact-revision":107735}
{"level":"info","ts":"2024-09-18T07:34:35.723874Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":108297}
{"level":"info","ts":"2024-09-18T07:34:35.72577Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":108297,"took":"805.7¬µs","hash":4076824591}
{"level":"info","ts":"2024-09-18T07:34:35.726267Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4076824591,"revision":108297,"compact-revision":107974}
{"level":"info","ts":"2024-09-18T07:38:53.978268Z","caller":"wal/wal.go:785","msg":"created a new WAL segment","path":"/var/lib/minikube/etcd/member/wal/0000000000000002-0000000000021406.wal"}
{"level":"info","ts":"2024-09-18T07:39:35.376991Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":108648}
{"level":"info","ts":"2024-09-18T07:39:35.379032Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":108648,"took":"820.9¬µs","hash":1304722981}
{"level":"info","ts":"2024-09-18T07:39:35.379204Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1304722981,"revision":108648,"compact-revision":108297}
{"level":"warn","ts":"2024-09-18T07:41:57.501835Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"182.4911ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031965687704083 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:109082 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >>","response":"size:18"}
{"level":"info","ts":"2024-09-18T07:41:57.50198Z","caller":"traceutil/trace.go:171","msg":"trace[1512054508] transaction","detail":"{read_only:false; response_revision:109096; number_of_response:1; }","duration":"193.8512ms","start":"2024-09-18T07:41:57.308101Z","end":"2024-09-18T07:41:57.501954Z","steps":["trace[1512054508] 'process raft request'  (duration: 192.8101ms)"],"step_count":1}
{"level":"info","ts":"2024-09-18T07:44:35.03741Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":108888}
{"level":"info","ts":"2024-09-18T07:44:35.039408Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":108888,"took":"1.6733ms","hash":3255143585}
{"level":"info","ts":"2024-09-18T07:44:35.03958Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3255143585,"revision":108888,"compact-revision":108648}
{"level":"info","ts":"2024-09-18T07:46:27.057504Z","caller":"traceutil/trace.go:171","msg":"trace[1628592599] transaction","detail":"{read_only:false; response_revision:109642; number_of_response:1; }","duration":"167.4927ms","start":"2024-09-18T07:46:26.889953Z","end":"2024-09-18T07:46:27.057451Z","steps":["trace[1628592599] 'process raft request'  (duration: 76.7474ms)"],"step_count":1}
{"level":"info","ts":"2024-09-18T07:49:34.683881Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":109420}
{"level":"info","ts":"2024-09-18T07:49:34.686249Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":109420,"took":"1.3517ms","hash":2453487360}
{"level":"info","ts":"2024-09-18T07:49:34.686349Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2453487360,"revision":109420,"compact-revision":108888}
{"level":"info","ts":"2024-09-18T07:54:34.329732Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":109876}
{"level":"info","ts":"2024-09-18T07:54:34.348054Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":109876,"took":"17.7052ms","hash":604653299}
{"level":"info","ts":"2024-09-18T07:54:34.34815Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":604653299,"revision":109876,"compact-revision":109420}
{"level":"info","ts":"2024-09-18T07:59:33.976445Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":110117}
{"level":"info","ts":"2024-09-18T07:59:33.977449Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":110117,"took":"577¬µs","hash":3078621381}
{"level":"info","ts":"2024-09-18T07:59:33.97762Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3078621381,"revision":110117,"compact-revision":109876}

* 
* ==> etcd [ccedc038ab1a] <==
* {"level":"warn","ts":"2024-09-17T14:42:11.559593Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-09-17T14:42:11.55994Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-09-17T14:42:11.560073Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-09-17T14:42:11.560146Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-09-17T14:42:11.56018Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-09-17T14:42:11.560329Z","caller":"embed/etcd.go:495","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-09-17T14:42:11.560755Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-09-17T14:42:11.561757Z","caller":"embed/etcd.go:309","msg":"starting an etcd server","etcd-version":"3.5.9","git-sha":"bdbbde998","go-version":"go1.19.9","go-os":"linux","go-arch":"amd64","max-cpu-set":4,"max-cpu-available":4,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-09-17T14:42:11.574727Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.7894ms"}
{"level":"warn","ts":"2024-09-17T14:42:11.575219Z","caller":"wal/util.go:90","msg":"ignored file in WAL directory","path":"0000000000000000-0000000000000000.wal.broken"}
{"level":"info","ts":"2024-09-17T14:42:13.158748Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":100010,"snapshot-size":"8.9 kB"}
{"level":"info","ts":"2024-09-17T14:42:13.158887Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":4501504,"backend-size":"4.5 MB","backend-size-in-use-bytes":1302528,"backend-size-in-use":"1.3 MB"}
{"level":"warn","ts":"2024-09-17T14:42:13.15899Z","caller":"wal/util.go:90","msg":"ignored file in WAL directory","path":"0000000000000000-0000000000000000.wal.broken"}
{"level":"info","ts":"2024-09-17T14:42:13.47152Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":107000}
{"level":"info","ts":"2024-09-17T14:42:13.472555Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-09-17T14:42:13.47271Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 7"}
{"level":"info","ts":"2024-09-17T14:42:13.472753Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 7, commit: 107000, applied: 100010, lastindex: 107000, lastterm: 7]"}
{"level":"info","ts":"2024-09-17T14:42:13.472964Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-09-17T14:42:13.473019Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-09-17T14:42:13.473048Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-09-17T14:42:13.475237Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-09-17T14:42:13.476504Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":85093}
{"level":"info","ts":"2024-09-17T14:42:13.478534Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":85456}
{"level":"info","ts":"2024-09-17T14:42:13.479878Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-09-17T14:42:13.481972Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-09-17T14:42:13.482636Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-09-17T14:42:13.482719Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-09-17T14:42:13.483277Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-09-17T14:42:13.483366Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-09-17T14:42:13.483611Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-09-17T14:42:13.483725Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-09-17T14:42:13.487069Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-09-17T14:42:13.487787Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-09-17T14:42:13.488052Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-09-17T14:42:13.489549Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-09-17T14:42:13.489407Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-09-17T14:42:14.073989Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 7"}
{"level":"info","ts":"2024-09-17T14:42:14.074432Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 7"}
{"level":"info","ts":"2024-09-17T14:42:14.075099Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 7"}
{"level":"info","ts":"2024-09-17T14:42:14.075632Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 8"}
{"level":"info","ts":"2024-09-17T14:42:14.075689Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 8"}
{"level":"info","ts":"2024-09-17T14:42:14.075745Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 8"}
{"level":"info","ts":"2024-09-17T14:42:14.075805Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 8"}
{"level":"info","ts":"2024-09-17T14:42:14.080912Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-09-17T14:42:14.080979Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-09-17T14:42:14.081318Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-09-17T14:42:14.082737Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-09-17T14:42:14.083312Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-09-17T14:42:14.083414Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-09-17T14:42:14.084184Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-09-17T14:44:44.257868Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-09-17T14:44:44.258003Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-09-17T14:44:44.258154Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-09-17T14:44:44.258329Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-09-17T14:44:44.385394Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-09-17T14:44:44.38561Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-09-17T14:44:44.38607Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-09-17T14:44:44.389698Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-09-17T14:44:44.390222Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-09-17T14:44:44.390262Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  07:59:39 up 20:15,  0 users,  load average: 1.21, 1.31, 1.08
Linux minikube 5.10.76-linuxkit #1 SMP Mon Nov 8 10:21:19 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [c5ab0f38ceeb] <==
* I0918 07:23:39.177681       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:24:39.110197       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:24:41.101495       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:25:39.031510       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:26:15.336934       1 alloc.go:330] "allocated clusterIPs" service="default/asso1" clusterIPs={"IPv4":"10.105.126.132"}
I0918 07:26:38.999768       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:27:19.132341       1 alloc.go:330] "allocated clusterIPs" service="default/nginx" clusterIPs={"IPv4":"10.99.127.201"}
I0918 07:27:38.887558       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:28:38.869381       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:29:38.744776       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:29:40.753503       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:30:38.669510       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:31:38.599603       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:32:08.156974       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0918 07:32:08.175587       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0918 07:32:38.522777       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:33:38.451591       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:34:38.376899       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:34:40.393680       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:35:38.305944       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:36:38.237449       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:37:38.165714       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:38:38.093698       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:39:38.021233       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:39:40.042890       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:40:37.949016       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:41:13.728833       1 alloc.go:330] "allocated clusterIPs" service="default/asso1" clusterIPs={"IPv4":"10.100.123.132"}
I0918 07:41:13.918597       1 alloc.go:330] "allocated clusterIPs" service="default/nginx" clusterIPs={"IPv4":"10.108.54.141"}
I0918 07:41:37.875371       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:42:37.801896       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:43:37.733147       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:44:17.352942       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0918 07:44:17.374621       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0918 07:44:28.823452       1 alloc.go:330] "allocated clusterIPs" service="default/k8bis" clusterIPs={"IPv4":"10.108.91.172"}
I0918 07:44:28.917796       1 alloc.go:330] "allocated clusterIPs" service="default/nginx" clusterIPs={"IPv4":"10.109.191.200"}
I0918 07:44:37.658275       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:44:39.721913       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:45:37.584462       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:46:20.501218       1 alloc.go:330] "allocated clusterIPs" service="default/asso1" clusterIPs={"IPv4":"10.108.178.199"}
I0918 07:46:37.514298       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:47:37.441458       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:48:37.368754       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:49:37.293592       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:49:39.362748       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:50:37.223168       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:51:37.149448       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:52:37.083018       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:53:37.003015       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:54:36.930374       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:54:39.017842       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:55:36.859250       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:56:36.785060       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:57:36.716881       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:58:26.471676       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0918 07:58:26.490968       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0918 07:58:34.304233       1 alloc.go:330] "allocated clusterIPs" service="default/asso1" clusterIPs={"IPv4":"10.110.155.137"}
I0918 07:58:34.528969       1 alloc.go:330] "allocated clusterIPs" service="default/nginx" clusterIPs={"IPv4":"10.107.121.118"}
I0918 07:58:36.645852       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:59:36.566576       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I0918 07:59:38.655968       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager

* 
* ==> kube-apiserver [e8bd370ad18e] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0917 14:44:54.234324       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0917 14:44:54.288167       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0917 14:44:54.337479       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0917 14:44:54.347900       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0917 14:44:54.365422       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0917 14:44:54.367207       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0917 14:44:54.447484       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [5f59ef5e65d4] <==
* I0918 07:46:52.297588       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="55.3¬µs"
I0918 07:46:55.176815       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="14.1898ms"
I0918 07:46:55.177713       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="71.5¬µs"
I0918 07:55:13.238433       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="16.2322ms"
I0918 07:55:13.238582       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="44.8¬µs"
I0918 07:55:23.710692       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="156.6¬µs"
I0918 07:55:37.824086       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="42.319ms"
I0918 07:55:37.825812       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="1.3192ms"
I0918 07:55:48.308135       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="20.2186ms"
I0918 07:55:48.311829       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="3.5277ms"
I0918 07:56:02.623676       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="69.7¬µs"
I0918 07:56:33.582681       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="11.8732ms"
I0918 07:56:33.583223       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="402.9¬µs"
I0918 07:58:17.298611       1 event.go:307] "Event occurred" object="default/asso1-c47fd9945" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: asso1-c47fd9945-tvsld"
I0918 07:58:17.331096       1 event.go:307] "Event occurred" object="default/k8bis-748ffccd6c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: k8bis-748ffccd6c-htxck"
I0918 07:58:17.393419       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="179.8679ms"
I0918 07:58:17.406418       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8bis-748ffccd6c" duration="152.6875ms"
I0918 07:58:17.435834       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="41.8194ms"
I0918 07:58:17.436648       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="56.6¬µs"
I0918 07:58:17.491863       1 event.go:307] "Event occurred" object="default/k8bis-748ffccd6c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: k8bis-748ffccd6c-tcf6c"
I0918 07:58:17.601872       1 event.go:307] "Event occurred" object="default/nginx-86c459c477" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nginx-86c459c477-ktn5k"
I0918 07:58:17.617084       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8bis-748ffccd6c" duration="210.4553ms"
I0918 07:58:17.689718       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="184.4686ms"
I0918 07:58:17.707978       1 event.go:307] "Event occurred" object="default/k8bis-748ffccd6c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: k8bis-748ffccd6c-89fx6"
I0918 07:58:17.787481       1 event.go:307] "Event occurred" object="default/nginx-86c459c477" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nginx-86c459c477-4pm22"
I0918 07:58:17.825380       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8bis-748ffccd6c" duration="208.1699ms"
I0918 07:58:17.998840       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="308.982ms"
I0918 07:58:18.015938       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8bis-748ffccd6c" duration="190.2941ms"
I0918 07:58:18.016076       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8bis-748ffccd6c" duration="44.6¬µs"
I0918 07:58:18.031259       1 event.go:307] "Event occurred" object="default/nginx-86c459c477" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nginx-86c459c477-b6pl8"
I0918 07:58:18.194677       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="195.7206ms"
I0918 07:58:18.305078       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="110.2884ms"
I0918 07:58:18.305262       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="55.6¬µs"
I0918 07:58:18.913038       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="14¬µs"
I0918 07:58:19.002119       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/k8bis-748ffccd6c" duration="25.2¬µs"
I0918 07:58:19.091345       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-7b5765f844" duration="67.2¬µs"
E0918 07:58:19.111664       1 garbagecollector.go:392] error syncing item &garbagecollector.node{identity:garbagecollector.objectReference{OwnerReference:v1.OwnerReference{APIVersion:"apps/v1", Kind:"ReplicaSet", Name:"nginx-7b5765f844", UID:"c5b3cd6a-bb1a-423e-b34e-9126a265749f", Controller:(*bool)(nil), BlockOwnerDeletion:(*bool)(nil)}, Namespace:"default"}, dependentsLock:sync.RWMutex{w:sync.Mutex{state:0, sema:0x0}, writerSem:0x0, readerSem:0x0, readerCount:atomic.Int32{_:atomic.noCopy{}, v:1}, readerWait:atomic.Int32{_:atomic.noCopy{}, v:0}}, dependents:map[*garbagecollector.node]struct {}{}, deletingDependents:false, deletingDependentsLock:sync.RWMutex{w:sync.Mutex{state:0, sema:0x0}, writerSem:0x0, readerSem:0x0, readerCount:atomic.Int32{_:atomic.noCopy{}, v:0}, readerWait:atomic.Int32{_:atomic.noCopy{}, v:0}}, beingDeleted:false, beingDeletedLock:sync.RWMutex{w:sync.Mutex{state:0, sema:0x0}, writerSem:0x0, readerSem:0x0, readerCount:atomic.Int32{_:atomic.noCopy{}, v:0}, readerWait:atomic.Int32{_:atomic.noCopy{}, v:0}}, virtual:false, virtualLock:sync.RWMutex{w:sync.Mutex{state:0, sema:0x0}, writerSem:0x0, readerSem:0x0, readerCount:atomic.Int32{_:atomic.noCopy{}, v:0}, readerWait:atomic.Int32{_:atomic.noCopy{}, v:0}}, owners:[]v1.OwnerReference{v1.OwnerReference{APIVersion:"apps/v1", Kind:"Deployment", Name:"nginx", UID:"ade56bf4-f27b-4e2c-9aa7-cc5a3e0fffb6", Controller:(*bool)(0xc001af1507), BlockOwnerDeletion:(*bool)(0xc001af1508)}}}: replicasets.apps "nginx-7b5765f844" not found
I0918 07:58:19.116049       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="17¬µs"
I0918 07:58:34.232487       1 event.go:307] "Event occurred" object="default/asso1" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set asso1-c47fd9945 to 1"
I0918 07:58:34.249411       1 event.go:307] "Event occurred" object="default/asso1-c47fd9945" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: asso1-c47fd9945-plmlg"
I0918 07:58:34.267179       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="37.4051ms"
I0918 07:58:34.299704       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="32.3315ms"
I0918 07:58:34.299979       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="43.7¬µs"
I0918 07:58:34.324263       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="71.4¬µs"
I0918 07:58:34.405501       1 event.go:307] "Event occurred" object="default/nginx" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set nginx-86c459c477 to 3"
I0918 07:58:34.420135       1 event.go:307] "Event occurred" object="default/nginx-86c459c477" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nginx-86c459c477-4lj7f"
I0918 07:58:34.490399       1 event.go:307] "Event occurred" object="default/nginx-86c459c477" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nginx-86c459c477-xvsfv"
I0918 07:58:34.498434       1 event.go:307] "Event occurred" object="default/nginx-86c459c477" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: nginx-86c459c477-lpxzt"
I0918 07:58:34.534242       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="130.8687ms"
I0918 07:58:34.614207       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="79.8103ms"
I0918 07:58:34.614493       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="49.3¬µs"
I0918 07:58:34.634148       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="336.5¬µs"
I0918 07:58:38.065171       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="7.9599ms"
I0918 07:58:38.065361       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="52.8¬µs"
I0918 07:58:39.108617       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="11.8695ms"
I0918 07:58:39.108848       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="88.2¬µs"
I0918 07:58:41.601530       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="66.3353ms"
I0918 07:58:41.601917       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/asso1-c47fd9945" duration="67.4¬µs"
I0918 07:58:42.536048       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="10.1025ms"
I0918 07:58:42.536304       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/nginx-86c459c477" duration="151.1¬µs"

* 
* ==> kube-controller-manager [a42cabb84c38] <==
* I0917 14:42:29.764976       1 shared_informer.go:311] Waiting for caches to sync for deployment
I0917 14:42:29.792348       1 controllermanager.go:642] "Started controller" controller="replicaset-controller"
I0917 14:42:29.792829       1 replica_set.go:214] "Starting controller" name="replicaset"
I0917 14:42:29.792972       1 shared_informer.go:311] Waiting for caches to sync for ReplicaSet
I0917 14:42:29.801813       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0917 14:42:29.829511       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0917 14:42:29.832312       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0917 14:42:29.833937       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0917 14:42:29.841624       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0917 14:42:29.892112       1 shared_informer.go:318] Caches are synced for expand
I0917 14:42:29.899065       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0917 14:42:29.899259       1 shared_informer.go:318] Caches are synced for service account
I0917 14:42:29.912600       1 shared_informer.go:318] Caches are synced for PV protection
I0917 14:42:29.919504       1 shared_informer.go:318] Caches are synced for PVC protection
I0917 14:42:29.922867       1 shared_informer.go:318] Caches are synced for crt configmap
I0917 14:42:29.924454       1 shared_informer.go:318] Caches are synced for disruption
I0917 14:42:29.932394       1 shared_informer.go:318] Caches are synced for namespace
I0917 14:42:29.936873       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0917 14:42:29.937079       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0917 14:42:29.937317       1 shared_informer.go:318] Caches are synced for stateful set
I0917 14:42:29.996283       1 shared_informer.go:318] Caches are synced for HPA
I0917 14:42:29.996461       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0917 14:42:29.996714       1 shared_informer.go:318] Caches are synced for deployment
I0917 14:42:29.996779       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="218.2¬µs"
I0917 14:42:29.997255       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="56.9¬µs"
I0917 14:42:29.997432       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="67.9¬µs"
I0917 14:42:29.997667       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="54.2¬µs"
I0917 14:42:29.998027       1 shared_informer.go:318] Caches are synced for attach detach
I0917 14:42:29.999572       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0917 14:42:30.000688       1 shared_informer.go:318] Caches are synced for ephemeral
I0917 14:42:30.007529       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0917 14:42:30.008759       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0917 14:42:30.011274       1 shared_informer.go:318] Caches are synced for ReplicationController
I0917 14:42:30.011641       1 shared_informer.go:318] Caches are synced for TTL
I0917 14:42:30.016529       1 shared_informer.go:318] Caches are synced for node
I0917 14:42:30.016791       1 range_allocator.go:174] "Sending events to api server"
I0917 14:42:30.017141       1 range_allocator.go:178] "Starting range CIDR allocator"
I0917 14:42:30.017172       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0917 14:42:30.017196       1 shared_informer.go:318] Caches are synced for cidrallocator
I0917 14:42:30.024460       1 shared_informer.go:318] Caches are synced for persistent volume
I0917 14:42:30.030018       1 shared_informer.go:318] Caches are synced for GC
I0917 14:42:30.052027       1 shared_informer.go:318] Caches are synced for resource quota
I0917 14:42:30.052428       1 shared_informer.go:318] Caches are synced for daemon sets
I0917 14:42:30.057450       1 shared_informer.go:318] Caches are synced for job
I0917 14:42:30.090076       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0917 14:42:30.090076       1 shared_informer.go:318] Caches are synced for taint
I0917 14:42:30.090983       1 shared_informer.go:318] Caches are synced for endpoint
I0917 14:42:30.092410       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0917 14:42:30.092389       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0917 14:42:30.096264       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0917 14:42:30.093768       1 taint_manager.go:211] "Sending events to api server"
I0917 14:42:30.095422       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0917 14:42:30.098153       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0917 14:42:30.098899       1 shared_informer.go:318] Caches are synced for cronjob
I0917 14:42:30.102146       1 shared_informer.go:318] Caches are synced for resource quota
I0917 14:42:30.106545       1 shared_informer.go:318] Caches are synced for TTL after finished
I0917 14:42:30.497483       1 shared_informer.go:318] Caches are synced for garbage collector
I0917 14:42:30.498091       1 shared_informer.go:318] Caches are synced for garbage collector
I0917 14:42:30.498145       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0917 14:42:46.564293       1 event.go:307] "Event occurred" object="kube-flannel/kube-flannel-ds" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-flannel-ds-4pd86"

* 
* ==> kube-proxy [756abc953a48] <==
* I0917 14:42:18.379143       1 server_others.go:69] "Using iptables proxy"
I0917 14:42:18.462957       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0917 14:42:18.652146       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0917 14:42:18.657740       1 server_others.go:152] "Using iptables Proxier"
I0917 14:42:18.657781       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0917 14:42:18.657802       1 server_others.go:438] "Defaulting to no-op detect-local"
I0917 14:42:18.657885       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0917 14:42:18.658355       1 server.go:846] "Version info" version="v1.28.3"
I0917 14:42:18.658399       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0917 14:42:18.660399       1 config.go:188] "Starting service config controller"
I0917 14:42:18.660441       1 shared_informer.go:311] Waiting for caches to sync for service config
I0917 14:42:18.660498       1 config.go:97] "Starting endpoint slice config controller"
I0917 14:42:18.660513       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0917 14:42:18.661303       1 config.go:315] "Starting node config controller"
I0917 14:42:18.661329       1 shared_informer.go:311] Waiting for caches to sync for node config
I0917 14:42:18.760677       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0917 14:42:18.760767       1 shared_informer.go:318] Caches are synced for service config
I0917 14:42:18.762538       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [b212bd94b61e] <==
* I0917 22:59:04.894001       1 server_others.go:69] "Using iptables proxy"
I0917 22:59:04.979645       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0917 22:59:05.158747       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0917 22:59:05.162820       1 server_others.go:152] "Using iptables Proxier"
I0917 22:59:05.163216       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0917 22:59:05.165645       1 server_others.go:438] "Defaulting to no-op detect-local"
I0917 22:59:05.166039       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0917 22:59:05.167736       1 server.go:846] "Version info" version="v1.28.3"
I0917 22:59:05.167908       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0917 22:59:05.170289       1 config.go:97] "Starting endpoint slice config controller"
I0917 22:59:05.170422       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0917 22:59:05.170495       1 config.go:188] "Starting service config controller"
I0917 22:59:05.170515       1 shared_informer.go:311] Waiting for caches to sync for service config
I0917 22:59:05.171766       1 config.go:315] "Starting node config controller"
I0917 22:59:05.171901       1 shared_informer.go:311] Waiting for caches to sync for node config
I0917 22:59:05.272599       1 shared_informer.go:318] Caches are synced for node config
I0917 22:59:05.272773       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0917 22:59:05.272957       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [5381e7606147] <==
* I0917 22:58:53.709549       1 serving.go:348] Generated self-signed cert in-memory
W0917 22:58:59.988012       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0917 22:58:59.989411       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0917 22:58:59.989564       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0917 22:58:59.989603       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0917 22:59:00.499634       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0917 22:59:00.499734       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0917 22:59:00.507992       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0917 22:59:00.510306       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0917 22:59:00.513127       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0917 22:59:00.517162       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0917 22:59:00.627827       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [cc443522d7d8] <==
* I0917 14:42:12.651627       1 serving.go:348] Generated self-signed cert in-memory
W0917 14:42:15.649797       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0917 14:42:15.649888       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0917 14:42:15.649933       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0917 14:42:15.649966       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0917 14:42:15.869109       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0917 14:42:15.869166       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0917 14:42:15.927038       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0917 14:42:15.927338       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0917 14:42:15.932292       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0917 14:42:15.932443       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0917 14:42:15.952055       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found]
E0917 14:42:15.952140       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found]
W0917 14:42:15.969048       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found]
E0917 14:42:15.969164       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found]
W0917 14:42:15.972077       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found]
E0917 14:42:15.973317       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found]
I0917 14:42:16.027758       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0917 14:44:44.382959       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0917 14:44:44.383172       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0917 14:44:44.383710       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0917 14:44:44.384133       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Sep 18 07:58:20 minikube kubelet[1547]: I0918 07:58:20.686593    1547 scope.go:117] "RemoveContainer" containerID="fd0b9454d7b33aeb27cbffe644364620109beffdfeeda105e2f947d843ada571"
Sep 18 07:58:20 minikube kubelet[1547]: I0918 07:58:20.723021    1547 scope.go:117] "RemoveContainer" containerID="1397b9d60ca38a06fca133e65ece64bcb26aa752d52452c2f6b06ae3da7eee53"
Sep 18 07:58:20 minikube kubelet[1547]: I0918 07:58:20.747566    1547 scope.go:117] "RemoveContainer" containerID="f08768e0471fa5d418a80955169f140d24d779869597deafd883ae220f3dd306"
Sep 18 07:58:20 minikube kubelet[1547]: I0918 07:58:20.767879    1547 scope.go:117] "RemoveContainer" containerID="f08768e0471fa5d418a80955169f140d24d779869597deafd883ae220f3dd306"
Sep 18 07:58:20 minikube kubelet[1547]: E0918 07:58:20.769383    1547 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: f08768e0471fa5d418a80955169f140d24d779869597deafd883ae220f3dd306" containerID="f08768e0471fa5d418a80955169f140d24d779869597deafd883ae220f3dd306"
Sep 18 07:58:20 minikube kubelet[1547]: I0918 07:58:20.769538    1547 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"f08768e0471fa5d418a80955169f140d24d779869597deafd883ae220f3dd306"} err="failed to get container status \"f08768e0471fa5d418a80955169f140d24d779869597deafd883ae220f3dd306\": rpc error: code = Unknown desc = Error response from daemon: No such container: f08768e0471fa5d418a80955169f140d24d779869597deafd883ae220f3dd306"
Sep 18 07:58:21 minikube kubelet[1547]: I0918 07:58:21.663474    1547 scope.go:117] "RemoveContainer" containerID="3811e4c19e728c6a36e02f3fef3e9f80f0f5bdbf55dca8f9d7d514c7e6c7d074"
Sep 18 07:58:22 minikube kubelet[1547]: I0918 07:58:22.429053    1547 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="16a40fc1-4090-42b0-9a88-5514b9494ff7" path="/var/lib/kubelet/pods/16a40fc1-4090-42b0-9a88-5514b9494ff7/volumes"
Sep 18 07:58:22 minikube kubelet[1547]: I0918 07:58:22.429843    1547 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="34eff2f3-9bde-4412-9720-627ac6fd0bd4" path="/var/lib/kubelet/pods/34eff2f3-9bde-4412-9720-627ac6fd0bd4/volumes"
Sep 18 07:58:22 minikube kubelet[1547]: I0918 07:58:22.430270    1547 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="de8cab7e-26cc-4490-87e3-8b1d3556dd56" path="/var/lib/kubelet/pods/de8cab7e-26cc-4490-87e3-8b1d3556dd56/volumes"
Sep 18 07:58:22 minikube kubelet[1547]: I0918 07:58:22.430778    1547 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="e0c8cb4f-16d1-40c1-bbfa-b9adadbf40aa" path="/var/lib/kubelet/pods/e0c8cb4f-16d1-40c1-bbfa-b9adadbf40aa/volumes"
Sep 18 07:58:22 minikube kubelet[1547]: I0918 07:58:22.431188    1547 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="fe360f8e-9043-46be-ad0b-58f36031b65d" path="/var/lib/kubelet/pods/fe360f8e-9043-46be-ad0b-58f36031b65d/volumes"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.264406    1547 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/k8bis-748ffccd6c-htxck" podStartSLOduration=15.7956357 podCreationTimestamp="2024-09-18 07:58:17 +0000 UTC" firstStartedPulling="2024-09-18 07:58:20.5099859 +0000 UTC m=+29338.676153001" lastFinishedPulling="2024-09-18 07:58:21.9787146 +0000 UTC m=+29340.144871501" observedRunningTime="2024-09-18 07:58:23.0987262 +0000 UTC m=+29341.264884001" watchObservedRunningTime="2024-09-18 07:58:34.2643542 +0000 UTC m=+29352.430514901"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.265422    1547 topology_manager.go:215] "Topology Admit Handler" podUID="7000eb36-38fb-44d2-befc-b10c877a11d2" podNamespace="default" podName="asso1-c47fd9945-plmlg"
Sep 18 07:58:34 minikube kubelet[1547]: E0918 07:58:34.265695    1547 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="34eff2f3-9bde-4412-9720-627ac6fd0bd4" containerName="k8bis"
Sep 18 07:58:34 minikube kubelet[1547]: E0918 07:58:34.265746    1547 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="de8cab7e-26cc-4490-87e3-8b1d3556dd56" containerName="k8bis"
Sep 18 07:58:34 minikube kubelet[1547]: E0918 07:58:34.265793    1547 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="0d19c675-b5b2-4886-a0f3-9e9c90ca565b" containerName="k8bis"
Sep 18 07:58:34 minikube kubelet[1547]: E0918 07:58:34.265825    1547 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="e0c8cb4f-16d1-40c1-bbfa-b9adadbf40aa" containerName="nginx"
Sep 18 07:58:34 minikube kubelet[1547]: E0918 07:58:34.265855    1547 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="fe360f8e-9043-46be-ad0b-58f36031b65d" containerName="nginx"
Sep 18 07:58:34 minikube kubelet[1547]: E0918 07:58:34.265884    1547 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="1ccab3fe-c382-4fa2-ad14-0911eee56f4a" containerName="asso1"
Sep 18 07:58:34 minikube kubelet[1547]: E0918 07:58:34.265912    1547 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="1ccab3fe-c382-4fa2-ad14-0911eee56f4a" containerName="asso1"
Sep 18 07:58:34 minikube kubelet[1547]: E0918 07:58:34.265935    1547 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="1ccab3fe-c382-4fa2-ad14-0911eee56f4a" containerName="asso1"
Sep 18 07:58:34 minikube kubelet[1547]: E0918 07:58:34.265965    1547 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="1ccab3fe-c382-4fa2-ad14-0911eee56f4a" containerName="asso1"
Sep 18 07:58:34 minikube kubelet[1547]: E0918 07:58:34.265989    1547 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="16a40fc1-4090-42b0-9a88-5514b9494ff7" containerName="nginx"
Sep 18 07:58:34 minikube kubelet[1547]: E0918 07:58:34.266013    1547 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="1ccab3fe-c382-4fa2-ad14-0911eee56f4a" containerName="asso1"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.266079    1547 memory_manager.go:346] "RemoveStaleState removing state" podUID="1ccab3fe-c382-4fa2-ad14-0911eee56f4a" containerName="asso1"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.266103    1547 memory_manager.go:346] "RemoveStaleState removing state" podUID="1ccab3fe-c382-4fa2-ad14-0911eee56f4a" containerName="asso1"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.266129    1547 memory_manager.go:346] "RemoveStaleState removing state" podUID="1ccab3fe-c382-4fa2-ad14-0911eee56f4a" containerName="asso1"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.266152    1547 memory_manager.go:346] "RemoveStaleState removing state" podUID="de8cab7e-26cc-4490-87e3-8b1d3556dd56" containerName="k8bis"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.266177    1547 memory_manager.go:346] "RemoveStaleState removing state" podUID="34eff2f3-9bde-4412-9720-627ac6fd0bd4" containerName="k8bis"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.266200    1547 memory_manager.go:346] "RemoveStaleState removing state" podUID="0d19c675-b5b2-4886-a0f3-9e9c90ca565b" containerName="k8bis"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.266224    1547 memory_manager.go:346] "RemoveStaleState removing state" podUID="16a40fc1-4090-42b0-9a88-5514b9494ff7" containerName="nginx"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.266246    1547 memory_manager.go:346] "RemoveStaleState removing state" podUID="e0c8cb4f-16d1-40c1-bbfa-b9adadbf40aa" containerName="nginx"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.266270    1547 memory_manager.go:346] "RemoveStaleState removing state" podUID="fe360f8e-9043-46be-ad0b-58f36031b65d" containerName="nginx"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.399552    1547 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sgjhd\" (UniqueName: \"kubernetes.io/projected/7000eb36-38fb-44d2-befc-b10c877a11d2-kube-api-access-sgjhd\") pod \"asso1-c47fd9945-plmlg\" (UID: \"7000eb36-38fb-44d2-befc-b10c877a11d2\") " pod="default/asso1-c47fd9945-plmlg"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.491393    1547 topology_manager.go:215] "Topology Admit Handler" podUID="56c04550-c267-4296-99e1-688ae4e5694d" podNamespace="default" podName="nginx-86c459c477-4lj7f"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.491616    1547 memory_manager.go:346] "RemoveStaleState removing state" podUID="1ccab3fe-c382-4fa2-ad14-0911eee56f4a" containerName="asso1"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.491651    1547 memory_manager.go:346] "RemoveStaleState removing state" podUID="1ccab3fe-c382-4fa2-ad14-0911eee56f4a" containerName="asso1"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.513399    1547 topology_manager.go:215] "Topology Admit Handler" podUID="5a352523-6d36-46e5-bbb8-a5a5c89bd8fa" podNamespace="default" podName="nginx-86c459c477-xvsfv"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.516648    1547 topology_manager.go:215] "Topology Admit Handler" podUID="d0f3fbb4-fa6c-4ce5-bc57-02b85b108574" podNamespace="default" podName="nginx-86c459c477-lpxzt"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.600534    1547 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2vcv7\" (UniqueName: \"kubernetes.io/projected/56c04550-c267-4296-99e1-688ae4e5694d-kube-api-access-2vcv7\") pod \"nginx-86c459c477-4lj7f\" (UID: \"56c04550-c267-4296-99e1-688ae4e5694d\") " pod="default/nginx-86c459c477-4lj7f"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.701302    1547 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4bc2r\" (UniqueName: \"kubernetes.io/projected/d0f3fbb4-fa6c-4ce5-bc57-02b85b108574-kube-api-access-4bc2r\") pod \"nginx-86c459c477-lpxzt\" (UID: \"d0f3fbb4-fa6c-4ce5-bc57-02b85b108574\") " pod="default/nginx-86c459c477-lpxzt"
Sep 18 07:58:34 minikube kubelet[1547]: I0918 07:58:34.701564    1547 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qhq5d\" (UniqueName: \"kubernetes.io/projected/5a352523-6d36-46e5-bbb8-a5a5c89bd8fa-kube-api-access-qhq5d\") pod \"nginx-86c459c477-xvsfv\" (UID: \"5a352523-6d36-46e5-bbb8-a5a5c89bd8fa\") " pod="default/nginx-86c459c477-xvsfv"
Sep 18 07:58:35 minikube kubelet[1547]: I0918 07:58:35.696898    1547 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="57ce9f6cf3bbe67638727ddb06f7bf709f191f303a6da8a154d073de19475dcc"
Sep 18 07:58:35 minikube kubelet[1547]: I0918 07:58:35.710664    1547 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e5a6f3756e5725845c76c6ec20efdfde5ce60877e10c8c8476149bce4f62fa24"
Sep 18 07:58:35 minikube kubelet[1547]: I0918 07:58:35.838989    1547 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="bd5a91618ecc0edd4b081e15944e538e324ae503abd8c4d6e53244eb6abd71dd"
Sep 18 07:58:35 minikube kubelet[1547]: I0918 07:58:35.915966    1547 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="fe8b5d0e96844fceefb64464e6b74e8f6ea3177140d43cbcd4cb4b24b197d966"
Sep 18 07:58:39 minikube kubelet[1547]: I0918 07:58:39.098710    1547 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/nginx-86c459c477-xvsfv" podStartSLOduration=3.7391549 podCreationTimestamp="2024-09-18 07:58:34 +0000 UTC" firstStartedPulling="2024-09-18 07:58:35.8033836 +0000 UTC m=+29353.969540301" lastFinishedPulling="2024-09-18 07:58:37.1628917 +0000 UTC m=+29355.329048601" observedRunningTime="2024-09-18 07:58:38.0587964 +0000 UTC m=+29356.224955801" watchObservedRunningTime="2024-09-18 07:58:39.0986632 +0000 UTC m=+29357.264821001"
Sep 18 07:58:39 minikube kubelet[1547]: I0918 07:58:39.099029    1547 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/nginx-86c459c477-4lj7f" podStartSLOduration=2.3475725 podCreationTimestamp="2024-09-18 07:58:34 +0000 UTC" firstStartedPulling="2024-09-18 07:58:35.8316208 +0000 UTC m=+29353.997788801" lastFinishedPulling="2024-09-18 07:58:38.5829377 +0000 UTC m=+29356.749098401" observedRunningTime="2024-09-18 07:58:39.0982184 +0000 UTC m=+29357.264380901" watchObservedRunningTime="2024-09-18 07:58:39.0988821 +0000 UTC m=+29357.265048001"
Sep 18 07:58:42 minikube kubelet[1547]: I0918 07:58:42.526124    1547 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/asso1-c47fd9945-plmlg" podStartSLOduration=4.5433278999999995 podCreationTimestamp="2024-09-18 07:58:34 +0000 UTC" firstStartedPulling="2024-09-18 07:58:35.9029991 +0000 UTC m=+29354.069156301" lastFinishedPulling="2024-09-18 07:58:39.8857394 +0000 UTC m=+29358.051898101" observedRunningTime="2024-09-18 07:58:41.5342235 +0000 UTC m=+29359.700382001" watchObservedRunningTime="2024-09-18 07:58:42.5260697 +0000 UTC m=+29360.728467701"
Sep 18 07:58:42 minikube kubelet[1547]: I0918 07:58:42.526581    1547 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/nginx-86c459c477-lpxzt" podStartSLOduration=3.2635363 podCreationTimestamp="2024-09-18 07:58:34 +0000 UTC" firstStartedPulling="2024-09-18 07:58:35.9538405 +0000 UTC m=+29354.120040401" lastFinishedPulling="2024-09-18 07:58:41.2168931 +0000 UTC m=+29359.383050801" observedRunningTime="2024-09-18 07:58:42.5257767 +0000 UTC m=+29360.728176101" watchObservedRunningTime="2024-09-18 07:58:42.5265467 +0000 UTC m=+29360.728944801"
Sep 18 07:58:53 minikube kubelet[1547]: I0918 07:58:53.412106    1547 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-t8bjv\" (UniqueName: \"kubernetes.io/projected/87a5bfc7-4bd3-4a60-be74-61558a294ce1-kube-api-access-t8bjv\") pod \"87a5bfc7-4bd3-4a60-be74-61558a294ce1\" (UID: \"87a5bfc7-4bd3-4a60-be74-61558a294ce1\") "
Sep 18 07:58:53 minikube kubelet[1547]: I0918 07:58:53.415520    1547 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/87a5bfc7-4bd3-4a60-be74-61558a294ce1-kube-api-access-t8bjv" (OuterVolumeSpecName: "kube-api-access-t8bjv") pod "87a5bfc7-4bd3-4a60-be74-61558a294ce1" (UID: "87a5bfc7-4bd3-4a60-be74-61558a294ce1"). InnerVolumeSpecName "kube-api-access-t8bjv". PluginName "kubernetes.io/projected", VolumeGidValue ""
Sep 18 07:58:53 minikube kubelet[1547]: I0918 07:58:53.513748    1547 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-t8bjv\" (UniqueName: \"kubernetes.io/projected/87a5bfc7-4bd3-4a60-be74-61558a294ce1-kube-api-access-t8bjv\") on node \"minikube\" DevicePath \"\""
Sep 18 07:58:53 minikube kubelet[1547]: I0918 07:58:53.761464    1547 scope.go:117] "RemoveContainer" containerID="b67e16d3fb17f42276fb27854f6882b756cb89a724a652e32f16be3d752284de"
Sep 18 07:58:53 minikube kubelet[1547]: I0918 07:58:53.786827    1547 scope.go:117] "RemoveContainer" containerID="b67e16d3fb17f42276fb27854f6882b756cb89a724a652e32f16be3d752284de"
Sep 18 07:58:53 minikube kubelet[1547]: E0918 07:58:53.789457    1547 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: b67e16d3fb17f42276fb27854f6882b756cb89a724a652e32f16be3d752284de" containerID="b67e16d3fb17f42276fb27854f6882b756cb89a724a652e32f16be3d752284de"
Sep 18 07:58:53 minikube kubelet[1547]: I0918 07:58:53.789554    1547 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"b67e16d3fb17f42276fb27854f6882b756cb89a724a652e32f16be3d752284de"} err="failed to get container status \"b67e16d3fb17f42276fb27854f6882b756cb89a724a652e32f16be3d752284de\": rpc error: code = Unknown desc = Error response from daemon: No such container: b67e16d3fb17f42276fb27854f6882b756cb89a724a652e32f16be3d752284de"
Sep 18 07:58:54 minikube kubelet[1547]: I0918 07:58:54.460030    1547 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="87a5bfc7-4bd3-4a60-be74-61558a294ce1" path="/var/lib/kubelet/pods/87a5bfc7-4bd3-4a60-be74-61558a294ce1/volumes"
Sep 18 07:59:22 minikube kubelet[1547]: W0918 07:59:22.442841    1547 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> kubernetes-dashboard [0667052406ba] <==
* 2024/09/17 22:59:52 Starting overwatch
2024/09/17 22:59:52 Using namespace: kubernetes-dashboard
2024/09/17 22:59:52 Using in-cluster config to connect to apiserver
2024/09/17 22:59:52 Using secret token for csrf signing
2024/09/17 22:59:52 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/09/17 22:59:52 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/09/17 22:59:52 Successful initial request to the apiserver, version: v1.28.3
2024/09/17 22:59:52 Generating JWE encryption key
2024/09/17 22:59:52 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/09/17 22:59:52 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/09/17 22:59:52 Initializing JWE encryption key from synchronized object
2024/09/17 22:59:52 Creating in-cluster Sidecar client
2024/09/17 22:59:52 Successful request to sidecar
2024/09/17 22:59:52 Serving insecurely on HTTP port: 9090

* 
* ==> kubernetes-dashboard [dac51631bdd9] <==
* 2024/09/17 22:59:07 Using namespace: kubernetes-dashboard
2024/09/17 22:59:07 Using in-cluster config to connect to apiserver
2024/09/17 22:59:07 Using secret token for csrf signing
2024/09/17 22:59:07 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/09/17 22:59:07 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: i/o timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00055fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc0001cec00)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf

* 
* ==> storage-provisioner [14d7245bce80] <==
* I0917 22:59:04.591074       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0917 22:59:34.618600       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> storage-provisioner [581f863026b2] <==
* I0917 22:59:46.216034       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0917 22:59:46.242575       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0917 22:59:46.242770       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0917 23:00:03.637683       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0917 23:00:03.638922       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"01e06e50-b39b-464d-b4ad-255887bb4222", APIVersion:"v1", ResourceVersion:"85882", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_fc9f21c3-0f12-4ff7-b9ff-e6486c8ca0e9 became leader
I0917 23:00:03.639283       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_fc9f21c3-0f12-4ff7-b9ff-e6486c8ca0e9!
I0917 23:00:03.741562       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_fc9f21c3-0f12-4ff7-b9ff-e6486c8ca0e9!

